{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mountain car source code: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Observation:\n",
    "    Type: Box(2)\n",
    "    Num    Observation               Min            Max\n",
    "    0      Car Position              -1.2           0.6\n",
    "    1      Car Velocity              -0.07          0.07\n",
    "    \n",
    "Actions:\n",
    "    Type: Discrete(3)\n",
    "    Num    Action\n",
    "    0      Accelerate to the Left\n",
    "    1      Don't accelerate\n",
    "    2      Accelerate to the Right\n",
    "    Note: This does not affect the amount of velocity affected by the\n",
    "    gravitational pull acting on the car.\n",
    "    \n",
    "Reward:\n",
    "     Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
    "     on top of the mountain.\n",
    "     Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "     \n",
    "Starting State:\n",
    "     The position of the car is assigned a uniform random value in\n",
    "     [-0.6 , -0.4].\n",
    "     The starting velocity of the car is always assigned to 0.\n",
    "     \n",
    "Episode Termination:\n",
    "     The car position is more than 0.5\n",
    "     Episode length is greater than 200\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\").env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "$f$ is input features.\n",
    "\n",
    "* $z_{0} = W_{0} * f + b_{0}$ --> (5x2) * (2x1) + (5x1) = (5x1)\n",
    "* $a_{0} = max(z_{0}, 0)$\n",
    "* $z_{1} = W_{1} * a_{0} + b_{1}$ --> (1x5) * (5x1) + (1x1) = (1x1)\n",
    "\n",
    "Derivatives:\n",
    "* $\\frac{\\partial z_{1}}{\\partial w_{0}} \n",
    "= \\frac{\\partial z_{1}}{\\partial a_{0}} \\frac{\\partial a_{0}}{\\partial z_{0}} \\frac{\\partial z_{0}}{\\partial w_{0}} \n",
    "= (w_{1}^{T} \\circ I_{z_{0}>0})f^{T}$ --> (5x1) * (1x2)\n",
    "* $\\frac{\\partial z_{1}}{\\partial b_{0}} \n",
    "= \\frac{\\partial z_{1}}{\\partial a_{0}} \\frac{\\partial a_{0}}{\\partial z_{0}} \\frac{\\partial z_{0}}{\\partial b_{0}} \n",
    "= (w_{1}^{T} \\circ I_{z_{0}>0})$ --> (5x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, step_size=0.01, epsilon=0.1, gamma=1):\n",
    "        self.action_space = env.action_space\n",
    "        self.gamma = gamma\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_action_value = None\n",
    "        self.last_state = None\n",
    "        self.last_features = None\n",
    "        \n",
    "        self.n_features = 5\n",
    "        self.n_hidden_nodes = 64\n",
    "        self.action_weights = [self.initialize_weights(), self.initialize_weights(), self.initialize_weights()]\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        weights = dict()\n",
    "        weights['w0'] = np.random.normal(loc=0, scale=0.707, size=(self.n_hidden_nodes, self.n_features))\n",
    "        weights['b0'] = np.zeros(shape=(self.n_hidden_nodes, 1))\n",
    "        weights['w1'] = np.random.normal(loc=0, scale=0.707, size=(1, self.n_hidden_nodes))\n",
    "        weights['b1'] = np.zeros(shape=(1, 1))\n",
    "        return weights\n",
    "    \n",
    "    def state_to_features(self, state):\n",
    "        features = state.copy()\n",
    "        features[1] *= 12  # very scientific normalization\n",
    "        features = np.append(features, features[0]*features[1])  # interaction term\n",
    "        features = np.append(features, features[0]*features[0])  # quadratic term\n",
    "        features = np.append(features, features[1]*features[1])  # quadratic term\n",
    "        return features.reshape((-1, 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def argmax(q_values):\n",
    "        max_value = np.max(q_values) - 1e-3\n",
    "        max_indices = np.where(q_values >= max_value)[0]\n",
    "        return np.random.choice(max_indices)\n",
    "    \n",
    "    def select_action(self, features):\n",
    "        action_values, a0s, z0s = self.network_all_actions(features)\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            action = self.argmax(action_values)\n",
    "        return action, action_values[action], a0s[action], z0s[action]\n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        features = self.state_to_features(state)\n",
    "        self.last_action, self.last_action_value, _, _ = self.select_action(features)\n",
    "        self.last_state = state\n",
    "        self.last_features = features\n",
    "        return self.last_action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        features = self.state_to_features(state)\n",
    "        action, action_value, a0, z0 = self.select_action(features)\n",
    "        # update weights\n",
    "        z1, a0, z0 = self.network(self.last_features, self.action_weights[self.last_action])\n",
    "        gradients = self.get_gradients(self.last_features, z1, a0, z0, self.action_weights[self.last_action])\n",
    "        self.action_weights[self.last_action] = self.update_weights(reward, action_value, self.action_weights[self.last_action], gradients)\n",
    "        # select next action\n",
    "        self.last_action = action\n",
    "        self.last_action_value = action_value\n",
    "        self.last_state = state\n",
    "        self.last_features = features\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        # update weights\n",
    "        action_value = 0  # end of episode\n",
    "        _, a0, z0 = self.network(self.last_features, self.action_weights[self.last_action])\n",
    "        gradients = self.get_gradients(self.last_features, action_value, a0, z0, self.action_weights[self.last_action])\n",
    "        self.action_weights[self.last_action] = self.update_weights(reward, action_value, self.action_weights[self.last_action], gradients)\n",
    "    \n",
    "    def network(self, features, weights):\n",
    "        \"\"\"features should be 2D array of shape (n_features, 1)\n",
    "        \"\"\"\n",
    "        z0 = np.dot(weights['w0'], features) + weights['b0']\n",
    "        a0 = np.maximum(z0, 0)  # ReLU\n",
    "        z1 = np.dot(weights['w1'], a0) + weights['b1']  # linear output layer\n",
    "        return z1, a0, z0\n",
    "    \n",
    "    def network_all_actions(self, features):\n",
    "        z1_0, a0_0, z0_0 = self.network(features, self.action_weights[0])\n",
    "        z1_1, a0_1, z0_1 = self.network(features, self.action_weights[1])\n",
    "        z1_2, a0_2, z0_2 = self.network(features, self.action_weights[2])\n",
    "        return [z1_0, z1_1, z1_2], [a0_0, a0_1, a0_2], [z0_0, z0_1, z0_2]\n",
    "    \n",
    "    def get_gradients(self, f, z1, a0, z0, weights):\n",
    "        grads = dict()\n",
    "        grads['d_w1'] = a0.T\n",
    "        grads['d_b1'] = np.ones(weights['b1'].shape)\n",
    "        grads['d_w0'] = np.dot(weights['w1'].T * (z0 > 0), f.T)\n",
    "        grads['d_b0'] = weights['w1'].T * (z0 > 0)\n",
    "        return grads\n",
    "        \n",
    "    def update_weights(self, reward, action_value, weights, grads):\n",
    "        td_error = reward + self.gamma * action_value - self.last_action_value\n",
    "        for name in ['w0', 'b0', 'w1', 'b0']:\n",
    "            weights[name] += self.step_size * td_error * grads['d_'+name]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(env, agent):\n",
    "    \"\"\"Run one (training) episode.\n",
    "    \"\"\"\n",
    "    step_nr = 0\n",
    "    last_observation = env.reset()\n",
    "    terminal = False\n",
    "    cumulative_reward = 0\n",
    "    state_list = []\n",
    "    agent.agent_start(last_observation.reshape((2,1)))\n",
    "    while not terminal and step_nr < 1000:\n",
    "        observation, reward, terminal, info = env.step(agent.last_action)\n",
    "        agent.agent_step(reward, observation.reshape((2,1)))\n",
    "        cumulative_reward += reward\n",
    "        state_list.append(observation)\n",
    "        step_nr += 1\n",
    "    agent.agent_end(reward)\n",
    "    return cumulative_reward, state_list\n",
    "\n",
    "\n",
    "def run_experiment(env, agent):\n",
    "    reward_list = []\n",
    "    state_lists = []\n",
    "    for i in tqdm(range(1000)):\n",
    "        episode_reward, state_list = episode(env, agent)\n",
    "        reward_list.append(episode_reward)\n",
    "        state_lists.append(state_list)\n",
    "    return reward_list, state_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, step_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, states = run_experiment(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rewards).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [s for lst in states for s in lst]\n",
    "states = np.array(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(states[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(states[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num=181)\n",
    "velocities = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state_values = pd.DataFrame(index=positions, columns=velocities, dtype='float32')\n",
    "df_top_action = pd.DataFrame(index=positions, columns=velocities)\n",
    "for i in positions:\n",
    "    for j in velocities:\n",
    "        features = agent.state_to_features(np.array([[i],[j]]))\n",
    "        action_values, _, _ = agent.network_all_actions(features)\n",
    "        top_action = np.argmax(action_values)\n",
    "        df_top_action.loc[i, j] = top_action\n",
    "        df_state_values.loc[i, j] = action_values[top_action][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_state_values.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_action.stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_top_action.astype(int).T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
