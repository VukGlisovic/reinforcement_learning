{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# average reward actor-critic with eligibility traces\n",
    "\n",
    "Cart Pole source code: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import rl.features.tile_coding as tc\n",
    "from rl.animations.video import write_video, show_video, simulate_episode\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "    a frictionless track. The pendulum starts upright, and the goal is to\n",
    "    prevent it from falling over by increasing and reducing the cart's\n",
    "    velocity.\n",
    "Observation:\n",
    "    Type: Box(4)\n",
    "    Num     Observation               Min                     Max\n",
    "    0       Cart Position             -4.8                    4.8\n",
    "    1       Cart Velocity             -Inf                    Inf\n",
    "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "    3       Pole Angular Velocity     -Inf                    Inf\n",
    "Actions:\n",
    "    Type: Discrete(2)\n",
    "    Num   Action\n",
    "    0     Push cart to the left\n",
    "    1     Push cart to the right\n",
    "    Note: The amount the velocity that is reduced or increased is not\n",
    "    fixed; it depends on the angle the pole is pointing. This is because\n",
    "    the center of gravity of the pole increases the amount of energy needed\n",
    "    to move the cart underneath it\n",
    "Reward:\n",
    "    Reward is 1 for every step taken, including the termination step\n",
    "Starting State:\n",
    "    All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "Episode Termination:\n",
    "    Pole Angle is more than 12 degrees.\n",
    "    Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "    the display).\n",
    "    Episode length is greater than 200.\n",
    "    Solved Requirements:\n",
    "    Considered solved when the average return is greater than or equal to\n",
    "    195.0 over 100 consecutive trials.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# quick experiment to see what the maximum velocities are that the environment really returns\n",
    "cart_vs = []\n",
    "pole_vs = []\n",
    "for i in range(3000):\n",
    "    cart_p, cart_v, pole_a, pole_v = env.reset()\n",
    "    cart_vs.append(cart_v)\n",
    "    pole_vs.append(pole_v)\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        (cart_p, cart_v, pole_a, pole_v), reward, terminal, _ = env.step(env.action_space.sample())\n",
    "        cart_vs.append(cart_v)\n",
    "        pole_vs.append(pole_v)\n",
    "min(cart_vs), max(cart_vs), min(pole_vs), max(pole_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, alpha_critic=0.01, lambda_critic=0.9, alpha_actor=0.01, lambda_actor=0.9, alpha_avg_reward=0.01, num_tilings=8, num_tiles=8, iht_size=4096):\n",
    "        self.action_space = env.action_space\n",
    "        self.actions = list(range(self.action_space.n))\n",
    "        self.alpha_critic = alpha_critic\n",
    "        self.lambda_critic = lambda_critic\n",
    "        self.alpha_actor = alpha_actor\n",
    "        self.lambda_actor = lambda_actor\n",
    "        self.alpha_avg_reward = alpha_avg_reward\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_softmax_probs = None\n",
    "        self.last_state = None\n",
    "        self.last_features = None\n",
    "        \n",
    "        self.iht_size = iht_size\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        \n",
    "        self.critic_w = np.ones(self.iht_size)\n",
    "        self.actor_w = [np.zeros(self.iht_size) for i in range(self.action_space.n)]\n",
    "        self.critic_z = None\n",
    "        self.actor_z = None\n",
    "        self.avg_reward = 0.\n",
    "    \n",
    "    def state_to_tiles(self, state):\n",
    "        cart_pos, cart_vel, pole_ang, pole_vel = state\n",
    "        cart_pos_scaled = ((cart_pos + 4.8) / 9.6) * self.num_tiles\n",
    "        cart_vel_scaled = ((cart_vel + 5) / 10) * self.num_tiles  # assuming max cart velocity of 5\n",
    "        pole_ang_scaled = ((pole_ang + 0.418) / 0.836) * self.num_tiles\n",
    "        pole_vel_scaled = ((pole_vel + 5) / 10) * self.num_tiles  # assuming max pole velocity of 5\n",
    "        return np.array(tc.tiles(self.iht, self.num_tilings, [cart_pos_scaled, cart_vel_scaled, pole_ang_scaled, pole_vel_scaled]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_stable(logits):\n",
    "        z = logits - max(logits)\n",
    "        num = np.exp(z)\n",
    "        den = np.sum(num)\n",
    "        return num / den\n",
    "    \n",
    "    def select_action(self, softmax_probs):\n",
    "        action = np.random.choice(self.actions, p=softmax_probs)\n",
    "        return action\n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        features = self.state_to_tiles(state)\n",
    "        action_logits = self.get_all_actor_logits(features)\n",
    "        softmax_probs = self.softmax_stable(action_logits)\n",
    "        self.last_action = self.select_action(softmax_probs)\n",
    "        self.last_softmax_probs = softmax_probs\n",
    "        self.last_state = state\n",
    "        self.last_features = features\n",
    "        self.critic_z = np.zeros(self.iht_size)\n",
    "        self.actor_z = [np.zeros(self.iht_size) for i in range(self.action_space.n)]\n",
    "        return self.last_action\n",
    "    \n",
    "    def agent_step(self, reward, state, terminal):\n",
    "        features = self.state_to_tiles(state)\n",
    "        action_logits = self.get_all_actor_logits(features)\n",
    "        softmax_probs = self.softmax_stable(action_logits)\n",
    "        action = self.select_action(softmax_probs)\n",
    "        # calculate delta\n",
    "        delta = reward - self.avg_reward + self.get_critic_estimate(features) * (1 - terminal) - self.get_critic_estimate(self.last_features)\n",
    "        # update average reward estimate\n",
    "        self.avg_reward += self.alpha_avg_reward * delta\n",
    "        # update eligibility trace vectors\n",
    "        self.critic_z[self.last_features] = self.lambda_critic * self.critic_z[self.last_features] + 1  # +1 from the critic gradient active tiles\n",
    "        self.critic_z[~self.last_features] = self.lambda_critic * self.critic_z[~self.last_features] + 0  # 0 since tiles are not active\n",
    "        other_action = (1 - self.last_action)\n",
    "        self.actor_z[self.last_action][self.last_features] = self.lambda_actor * self.actor_z[self.last_action][self.last_features] + (1 - self.last_softmax_probs[self.last_action])\n",
    "        self.actor_z[self.last_action][~self.last_features] = self.lambda_actor * self.actor_z[self.last_action][~self.last_features] + 0\n",
    "        self.actor_z[other_action][self.last_features] = self.lambda_actor * self.actor_z[other_action][self.last_features] + (0 - self.last_softmax_probs[other_action])\n",
    "        self.actor_z[other_action][~self.last_features] = self.lambda_actor * self.actor_z[other_action][~self.last_features] + 0\n",
    "        # update critic weights\n",
    "        self.critic_w += self.alpha_critic * delta * self.critic_z\n",
    "        # update actor weights\n",
    "        self.actor_w[self.last_action] += self.alpha_actor * delta * self.actor_z[self.last_action]\n",
    "        self.actor_w[other_action] += self.alpha_actor * delta * self.actor_z[other_action]\n",
    "        # select next action\n",
    "        self.last_action = action\n",
    "        self.last_softmax_probs = softmax_probs\n",
    "        self.last_state = state\n",
    "        self.last_features = features\n",
    "        \n",
    "    def get_critic_estimate(self, features):\n",
    "        return self.critic_w[features].sum()\n",
    "    \n",
    "    def get_actor_logit(self, features, action):\n",
    "        return self.actor_w[action][features].sum()\n",
    "    \n",
    "    def get_all_actor_logits(self, features):\n",
    "        action_values = []\n",
    "        for a in range(self.action_space.n):\n",
    "            action_values.append(self.get_actor_logit(features, a))\n",
    "        return np.array(action_values)\n",
    "    \n",
    "    def state_to_action(self, state):\n",
    "        features = self.state_to_tiles(state)\n",
    "        action_logits = self.get_all_actor_logits(features)\n",
    "        softmax_probs = self.softmax_stable(action_logits)\n",
    "        return self.select_action(softmax_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(env, agent):\n",
    "    \"\"\"Run one (training) episode.\n",
    "    \"\"\"\n",
    "    last_observation = env.reset()\n",
    "    terminal = False\n",
    "    cumulative_reward = 0\n",
    "    state_list = []\n",
    "    agent.agent_start(last_observation)\n",
    "    while not terminal:\n",
    "        observation, reward, terminal, info = env.step(agent.last_action)\n",
    "        cumulative_reward += reward\n",
    "        state_list.append(observation)\n",
    "        if info.get('TimeLimit.truncated'):\n",
    "            # environment truncated the episode, therefore terminal is actually False\n",
    "            agent.agent_step(reward, observation, False)\n",
    "        else:\n",
    "            agent.agent_step(reward, observation, terminal)\n",
    "    return cumulative_reward, state_list\n",
    "\n",
    "\n",
    "def run_experiment(env, agent, n_episodes=1000, title=None):\n",
    "    reward_list = []\n",
    "    state_lists = []\n",
    "    pbar = tqdm(range(n_episodes))\n",
    "    for i in pbar:\n",
    "        if title:\n",
    "            pbar.set_description(title)\n",
    "        episode_reward, state_list = episode(env, agent)\n",
    "        reward_list.append(episode_reward)\n",
    "        state_lists.append(state_list)\n",
    "    return reward_list, state_lists\n",
    "\n",
    "\n",
    "def animate_episode(env, agent):\n",
    "    if agent == 'random':\n",
    "        action_fnc = lambda x: env.action_space.sample()\n",
    "    else:\n",
    "        action_fnc = lambda x: agent.state_to_action(x)\n",
    "    obs = env.reset()\n",
    "    terminal = False\n",
    "    i = 0\n",
    "    while not terminal and i < 500:\n",
    "        obs, _, terminal, _ = env.step(action_fnc(obs))\n",
    "        env.render()\n",
    "        i += 1\n",
    "        time.sleep(1/30)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random agent\n",
    "simulate_episode(env, lambda x: env.action_space.sample(), width=500, play_type='autoplay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the experiment with hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'alpha_critic': 0.0625, 'alpha_actor': 0.0078125, 'alpha_avg_reward': 0.0078125, 'lambda_critic': 0.5, 'lambda_actor': 0.8}\n",
    "grid = {\n",
    "    'alpha_critic': [2**-4, 2**-5],\n",
    "    'alpha_actor': [2**-7, 2**-8],\n",
    "    'alpha_avg_reward': [2**-6, 2**-7],\n",
    "    'lambda_critic': [0.8, 0.5],\n",
    "    'lambda_actor': [0.8, 0.5]\n",
    "}\n",
    "\n",
    "\n",
    "def get_hyperparameter_dict(grid):\n",
    "    param_names = list(grid.keys())\n",
    "    combinations = product(*[grid[name] for name in param_names])\n",
    "    for values in combinations:\n",
    "        yield dict(zip(param_names, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for i, agent_setup in enumerate(get_hyperparameter_dict(grid)):\n",
    "    agent = Agent(env, **agent_setup)\n",
    "    rewards, observations = run_experiment(env, agent, n_episodes=1000, title='Agent Setup {}: {}'.format(i, agent_setup))\n",
    "    observations = np.array([s for lst in observations for s in lst])\n",
    "    results['run' + str(i)] = {'agent': agent, 'agent_setup': agent_setup, 'rewards': rewards, 'observations': observations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "ax.set_title(\"Rolling mean of total reward per episode\", fontsize=20)\n",
    "ax.set_xlabel(\"Episode Number\", fontsize=16)\n",
    "ax.set_ylabel(\"Reward per Episode\", fontsize=16)\n",
    "for run in results.keys():\n",
    "    ax.plot(pd.Series(results[run]['rewards']).rolling(window=50).mean(), label=results[run]['agent_setup'])\n",
    "ax.legend(loc='center', bbox_to_anchor=(0.5, -(len(results) / 40)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in results.keys():\n",
    "    print(results[run]['agent_setup'])\n",
    "    print(pd.Series(results[run]['rewards']).rolling(window=50).mean().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best agent based on the latest rewards\n",
    "best_average_reward = -np.inf\n",
    "for run in results.keys():\n",
    "    final_average_reward = np.mean(results[run]['rewards'][-50:])\n",
    "    if final_average_reward > best_average_reward:\n",
    "        best_average_reward = final_average_reward\n",
    "        best_agent = results[run]['agent']\n",
    "        observations = results[run]['observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained agent\n",
    "simulate_episode(env.env, best_agent.state_to_action, max_steps=2000, width=600, play_type='autoplay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
