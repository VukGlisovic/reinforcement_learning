{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cart Pole source code: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import rl.features.tile_coding as tc\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "    a frictionless track. The pendulum starts upright, and the goal is to\n",
    "    prevent it from falling over by increasing and reducing the cart's\n",
    "    velocity.\n",
    "Observation:\n",
    "    Type: Box(4)\n",
    "    Num     Observation               Min                     Max\n",
    "    0       Cart Position             -4.8                    4.8\n",
    "    1       Cart Velocity             -Inf                    Inf\n",
    "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "    3       Pole Angular Velocity     -Inf                    Inf\n",
    "Actions:\n",
    "    Type: Discrete(2)\n",
    "    Num   Action\n",
    "    0     Push cart to the left\n",
    "    1     Push cart to the right\n",
    "    Note: The amount the velocity that is reduced or increased is not\n",
    "    fixed; it depends on the angle the pole is pointing. This is because\n",
    "    the center of gravity of the pole increases the amount of energy needed\n",
    "    to move the cart underneath it\n",
    "Reward:\n",
    "    Reward is 1 for every step taken, including the termination step\n",
    "Starting State:\n",
    "    All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "Episode Termination:\n",
    "    Pole Angle is more than 12 degrees.\n",
    "    Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "    the display).\n",
    "    Episode length is greater than 200.\n",
    "    Solved Requirements:\n",
    "    Considered solved when the average return is greater than or equal to\n",
    "    195.0 over 100 consecutive trials.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick experiment to see what the maximum velocities are that the environment really returns\n",
    "cart_vs = []\n",
    "pole_vs = []\n",
    "for i in range(3000):\n",
    "    cart_p, cart_v, pole_a, pole_v = env.reset()\n",
    "    cart_vs.append(cart_v)\n",
    "    pole_vs.append(pole_v)\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        (cart_p, cart_v, pole_a, pole_v), reward, terminal, _ = env.step(env.action_space.sample())\n",
    "        cart_vs.append(cart_v)\n",
    "        pole_vs.append(pole_v)\n",
    "min(cart_vs), max(cart_vs), min(pole_vs), max(pole_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "$\\pi(A_{t}=a|S_{t}=s,\\theta) = \\theta^{T} * x(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, alpha_critic=0.01, alpha_actor=0.01, alpha_avg_reward=0.01, num_tilings=8, num_tiles=8, iht_size=4096):\n",
    "        self.action_space = env.action_space\n",
    "        self.actions = list(range(self.action_space.n))\n",
    "        self.alpha_avg_reward = alpha_avg_reward\n",
    "        self.alpha_critic = alpha_critic\n",
    "        self.alpha_actor = alpha_actor\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_softmax_probs = None\n",
    "        self.last_state = None\n",
    "        self.last_features = None\n",
    "        \n",
    "        self.iht_size = iht_size\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        \n",
    "        self.critic_w = np.ones(self.iht_size)\n",
    "        self.actor_w = [np.zeros(self.iht_size) for i in range(self.action_space.n)]\n",
    "        self.avg_reward = 0.\n",
    "    \n",
    "    def state_to_tiles(self, state):\n",
    "        cart_pos, cart_vel, pole_ang, pole_vel = state\n",
    "        cart_pos_scaled = ((cart_pos + 4.8) / 9.6) * self.num_tiles\n",
    "        cart_vel_scaled = ((cart_vel + 5) / 10) * self.num_tiles  # assuming max cart velocity of 5\n",
    "        pole_ang_scaled = ((pole_ang + 0.418) / 0.836) * self.num_tiles\n",
    "        pole_vel_scaled = ((pole_vel + 5) / 10) * self.num_tiles  # assuming max pole velocity of 5\n",
    "        return np.array(tc.tiles(self.iht, self.num_tilings, [cart_pos_scaled, cart_vel_scaled, pole_ang_scaled, pole_vel_scaled]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_stable(logits):\n",
    "        z = logits - max(logits)\n",
    "        num = np.exp(z)\n",
    "        den = np.sum(num)\n",
    "        return num / den\n",
    "    \n",
    "    def select_action(self, softmax_probs):\n",
    "        action = np.random.choice(self.actions, p=softmax_probs)\n",
    "        return action\n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        features = self.state_to_tiles(state)\n",
    "        action_logits = self.get_all_actor_logits(features)\n",
    "        softmax_probs = self.softmax_stable(action_logits)\n",
    "        self.last_action = self.select_action(softmax_probs)\n",
    "        self.last_softmax_probs = softmax_probs\n",
    "        self.last_state = state\n",
    "        self.last_features = features\n",
    "        return self.last_action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        features = self.state_to_tiles(state)\n",
    "        action_logits = self.get_all_actor_logits(features)\n",
    "        softmax_probs = self.softmax_stable(action_logits)\n",
    "        action = self.select_action(softmax_probs)\n",
    "        # calculate delta\n",
    "        delta = reward - self.avg_reward + self.get_critic_estimate(features) - self.get_critic_estimate(self.last_features)\n",
    "        # update average reward estimate\n",
    "        self.avg_reward += self.alpha_avg_reward * delta\n",
    "        # update critic weights\n",
    "        self.critic_w[self.last_features] += self.alpha_critic * delta  # *1 for the gradient officially\n",
    "        # update actor weights\n",
    "        self.actor_w[self.last_action][self.last_features] += self.alpha_actor * delta * (1 - self.last_softmax_probs[self.last_action])  # selected action\n",
    "        other_action = (1 - self.last_action)\n",
    "        self.actor_w[other_action][self.last_features] += self.alpha_actor * delta * (0 - self.last_softmax_probs[other_action])  # other action\n",
    "        # select next action\n",
    "        self.last_action = action\n",
    "        self.last_softmax_probs = softmax_probs\n",
    "        self.last_state = state\n",
    "        self.last_features = features\n",
    "        \n",
    "    def get_critic_estimate(self, features):\n",
    "        return self.critic_w[features].sum()\n",
    "    \n",
    "    def get_actor_logit(self, features, action):\n",
    "        return self.actor_w[action][features].sum()\n",
    "    \n",
    "    def get_all_actor_logits(self, features):\n",
    "        action_values = []\n",
    "        for a in range(self.action_space.n):\n",
    "            action_values.append(self.get_actor_logit(features, a))\n",
    "        return np.array(action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(env, agent):\n",
    "    \"\"\"Run one (training) episode.\n",
    "    \"\"\"\n",
    "    last_observation = env.reset()\n",
    "    terminal = False\n",
    "    cumulative_reward = 0\n",
    "    state_list = []\n",
    "    agent.agent_start(last_observation)\n",
    "    while not terminal:\n",
    "        observation, reward, terminal, info = env.step(agent.last_action)\n",
    "        agent.agent_step(reward, observation)\n",
    "        cumulative_reward += reward\n",
    "        state_list.append(observation)\n",
    "    return cumulative_reward, state_list\n",
    "\n",
    "\n",
    "def run_experiment(env, agent, n_episodes=1000):\n",
    "    reward_list = []\n",
    "    state_lists = []\n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        episode_reward, state_list = episode(env, agent)\n",
    "        reward_list.append(episode_reward)\n",
    "        state_lists.append(state_list)\n",
    "    return reward_list, state_lists\n",
    "\n",
    "\n",
    "def animate_episode(env, agent):\n",
    "    if agent == 'random':\n",
    "        action_fnc = lambda x: env.action_space.sample()\n",
    "    elif agent == 'right':\n",
    "        action_fnc = lambda x: 2\n",
    "    else:\n",
    "        action_fnc = lambda x: agent.select_action(agent.state_to_tiles(x))[0]\n",
    "    obs = env.reset()\n",
    "    terminal = False\n",
    "    i = 0\n",
    "    while not terminal and i < 500:\n",
    "        obs, _, terminal, _ = env.step(action_fnc(obs))\n",
    "        env.render()\n",
    "        i += 1\n",
    "        time.sleep(1/30)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example episodes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# random agent\n",
    "animate_episode(env, 'random')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# agent that only goes right\n",
    "animate_episode(env, 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, alpha_critic=0.0625, alpha_actor=0.0078, alpha_avg_reward=0.0156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, observations = run_experiment(env, agent, n_episodes=2000)\n",
    "observations = np.array([s for lst in observations for s in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "plt.plot(pd.Series(rewards).rolling(window=50).mean())\n",
    "ax.set_title(\"Rolling mean of total reward per episode\", fontsize=20)\n",
    "ax.set_xlabel(\"Episode Number\", fontsize=16)\n",
    "ax.set_ylabel(\"Reward per Episode\", fontsize=16);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# trained agent\n",
    "animate_episode(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.DataFrame(observations[:,[0,2]], columns=['cart_pos', 'pole_ang'])\n",
    "cart_pos_linspace = np.linspace(-4.8, 4.8, 97)\n",
    "pole_ang_linspace = np.linspace(-0.418, 0.418, 97)\n",
    "states['cart_pos_bin'] = pd.IntervalIndex(pd.cut(states['cart_pos'], cart_pos_linspace)).mid\n",
    "states['pole_ang_bin'] = pd.IntervalIndex(pd.cut(states['pole_ang'], pole_ang_linspace)).mid\n",
    "state_visitation_counts = states.groupby(['cart_pos_bin', 'pole_ang_bin'])['cart_pos'].count().unstack(fill_value=0)\n",
    "state_visitation_counts.index = np.round(state_visitation_counts.index, 3)\n",
    "state_visitation_counts.columns = np.round(state_visitation_counts.columns, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create heatmap of visitation count\n",
    "fig, ax = plt.subplots(figsize=(18, 9))\n",
    "sns.heatmap(np.log(state_visitation_counts + 1).T.sort_index(ascending=False), ax=ax)\n",
    "ax.set_title(\"Natural log state visitation counts\", fontsize=20)\n",
    "ax.set_xlabel(\"cart position\", fontsize=16)\n",
    "ax.set_ylabel(\"pole angle\", fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frames with state values and top action\n",
    "cart_pos_linspace = np.linspace(-4.8, 4.8, 97)\n",
    "pole_ang_linspace = np.linspace(-0.418, 0.418, 97)\n",
    "df_state_values = pd.DataFrame(index=cart_pos_linspace, columns=pole_ang_linspace, dtype='float32')\n",
    "for i in cart_pos_linspace:\n",
    "    for j in pole_ang_linspace:\n",
    "        features = agent.state_to_tiles(np.array([i,0,j,0]))\n",
    "        df_state_values.loc[i, j] = agent.get_critic_estimate(features)\n",
    "df_state_values.index = np.round(df_state_values.index, 3)\n",
    "df_state_values.columns = np.round(df_state_values.columns, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create heatmaps of state values and top action\n",
    "fig, ax = plt.subplots(figsize=(18, 9))\n",
    "sns.heatmap(df_state_values.T.sort_index(ascending=False), ax=ax)\n",
    "ax.set_title(\"State value estimates\", fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
