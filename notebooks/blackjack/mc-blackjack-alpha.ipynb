{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blackjack environment\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py\n",
    "\n",
    "<img src=\"https://www.blackjack.org/wp-content/uploads/2018/12/Blackjack-values.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gym.make('Blackjack-v0')  # makes sure BlackJackEnv is imported\n",
    "\n",
    "\n",
    "class BlackJack(gym.envs.toy_text.BlackjackEnv):\n",
    "    \"\"\"\n",
    "    The observation space of the blackjack environment is a 3-tuple containing \n",
    "    the following information: (sum of players hand, dealer his showing card, usable ace)\n",
    "    action space contains just actions: (0: stick, 1: hit)\n",
    "    \n",
    "    The player can request additional cards (hit) until they decide to stop\n",
    "    (stick) or exceed 21 (bust).\n",
    "    \n",
    "    The dealer draws cards until their sum is 17 or greater.\n",
    "\n",
    "    If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
    "    decided by whose sum is closer to 21. The reward for winning is +1,\n",
    "    drawing is 0, and losing is -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        sum_hand, dealer_card, usable_ace = super()._get_obs()\n",
    "        return (sum_hand, dealer_card, int(usable_ace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, observation_space, action_space, discount=1., alpha=0.01):\n",
    "        self.q_values = np.zeros([space.n for space in observation_space] + [action_space.n])\n",
    "        self.policy = self.initialize_policy(observation_space)\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def initialize_policy(self, observation_space):\n",
    "        \"\"\"Initial policy is to hit whenever the sum in the hand is 19 or less,\n",
    "        regardless of what the dealer showing card is.\n",
    "        \"\"\"\n",
    "        policy = np.zeros([space.n for space in observation_space], dtype='int16')\n",
    "        policy[:20,:,:] = 1\n",
    "        return policy\n",
    "    \n",
    "    def get_action(self, sum_hand, dealer_card, usable_ace):\n",
    "        return self.policy[sum_hand, dealer_card, usable_ace]\n",
    "    \n",
    "    def estimate_q_values_and_update_policy(self, episode_history):\n",
    "        # calculate the returns for the visited states\n",
    "        G = 0\n",
    "        returns = dict()\n",
    "        # replay the episode backwards\n",
    "        for obs, a, r in episode_history[::-1]:\n",
    "            # obs is tuple with: ('sum of players hand', 'dealer his showing card', 'usable ace')\n",
    "            G = self.discount * G + r\n",
    "            returns[obs+(a,)] = G\n",
    "        # update q value estimates and update the policy\n",
    "        for k,G in returns.items():\n",
    "            # k is tuple with: ('sum of players hand', 'dealer his showing card', 'usable ace', 'action taken')\n",
    "            self.q_values[k] = self.q_values[k] + self.alpha * (G - self.q_values[k])\n",
    "            self.policy[k[:3]] = np.argmax(self.q_values[k[:3]])\n",
    "\n",
    "    def render_policy(self):\n",
    "        fig = plt.figure(figsize=(10, 8), constrained_layout=True)\n",
    "        spec = fig.add_gridspec(ncols=2, nrows=2, width_ratios=[1, 1], height_ratios=[5, 3])\n",
    "        ax1 = fig.add_subplot(spec[:, :-1])\n",
    "        ax2 = fig.add_subplot(spec[:-1, 1:])\n",
    "\n",
    "        sns.heatmap(self.policy[4:22,1:11,0], cbar=False, linewidths=1, linecolor='grey', ax=ax1)\n",
    "        ax1.invert_yaxis()\n",
    "        ax1.set_title(\"No Usable Ace\", fontsize=18)\n",
    "        ax1.set_xlabel(\"Dealer Showing\", fontsize=16)\n",
    "        ax1.set_xticklabels(['A'] + list(range(2, 11)), fontsize=14)\n",
    "        ax1.set_ylabel(\"Sum Hand\", fontsize=16)\n",
    "        ax1.set_yticklabels(range(4, 22), fontsize=14)\n",
    "        ax1.text(5, 3, \"HIT\", ha='center', va='center', fontsize=56)\n",
    "        ax1.text(5, 16.5, \"STICK\", ha='center', va='center', fontsize=56, color='white')\n",
    "\n",
    "        sns.heatmap(self.policy[12:22,1:11,1], cbar=False, linewidths=1, linecolor='grey', ax=ax2)\n",
    "        ax2.invert_yaxis()\n",
    "        ax2.set_title(\"Usable Ace\", fontsize=18)\n",
    "        ax2.set_xlabel(\"Dealer Showing\", fontsize=16)\n",
    "        ax2.set_xticklabels(['A'] + list(range(2, 11)), fontsize=14)\n",
    "        ax2.set_ylabel(\"Sum Hand\", fontsize=16)\n",
    "        ax2.set_yticklabels(range(12, 22), fontsize=14)\n",
    "        ax2.text(5, 2.5, \"HIT\", ha='center', va='center', fontsize=56)\n",
    "        ax2.text(5, 8.5, \"STICK\", ha='center', va='center', fontsize=56, color='white');\n",
    "\n",
    "    def render_q_values(self, hit_or_stick):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hit_or_stick (int): 0=stick, 1=hit\n",
    "        \"\"\"\n",
    "        action = 'HIT' if hit_or_stick else 'STICK'\n",
    "        fig = plt.figure(figsize=(10, 8), constrained_layout=True)\n",
    "        spec = fig.add_gridspec(ncols=2, nrows=2, width_ratios=[1, 1], height_ratios=[5, 3])\n",
    "        ax1 = fig.add_subplot(spec[:, :-1])\n",
    "        ax2 = fig.add_subplot(spec[:-1, 1:])\n",
    "        fig.text(0.5, 1.05, \"Action values for {}\".format(action), fontsize=20, ha='center', va='center')\n",
    "\n",
    "        sns.heatmap(self.q_values[4:22-hit_or_stick,1:11,0,hit_or_stick], vmin=-1, vmax=1, cmap='coolwarm', cbar=True, linewidths=1, linecolor='grey', ax=ax1)\n",
    "        ax1.invert_yaxis()\n",
    "        ax1.set_title(\"No Usable Ace\", fontsize=18)\n",
    "        ax1.set_xlabel(\"Dealer Showing\", fontsize=16)\n",
    "        ax1.set_xticklabels(['A'] + list(range(2, 11)), fontsize=14)\n",
    "        ax1.set_ylabel(\"Sum Hand\", fontsize=16)\n",
    "        ax1.set_yticklabels(range(4, 22-hit_or_stick), fontsize=14)\n",
    "\n",
    "        sns.heatmap(self.q_values[12:22,1:11,1,hit_or_stick], vmin=-1, vmax=1, cmap='coolwarm', cbar=False, linewidths=1, linecolor='grey', ax=ax2)\n",
    "        ax2.invert_yaxis()\n",
    "        ax2.set_title(\"Usable Ace\", fontsize=18)\n",
    "        ax2.set_xlabel(\"Dealer Showing\", fontsize=16)\n",
    "        ax2.set_xticklabels(['A'] + list(range(2, 11)), fontsize=14)\n",
    "        ax2.set_ylabel(\"Sum Hand\", fontsize=16)\n",
    "        ax2.set_yticklabels(range(12, 22), fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode():\n",
    "    last_observation = env.reset()\n",
    "    sum_hand, dealer_card, usable_ace = last_observation\n",
    "    last_action = env.action_space.sample()  # initial action is random (exploring starts)\n",
    "    terminal = False\n",
    "    history = []\n",
    "    while not terminal:\n",
    "        observation, reward, terminal, info = env.step(last_action)\n",
    "        sum_hand, dealer_card, usable_ace = observation\n",
    "        history += [(last_observation, last_action, reward)]\n",
    "        last_observation = observation\n",
    "        last_action = agent.get_action(sum_hand, dealer_card, usable_ace)  # next action based on policy\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment and agent and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackJack()\n",
    "agent = Agent(env.observation_space, env.action_space, alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(2500000)):\n",
    "    history = episode()\n",
    "    agent.estimate_q_values_and_update_policy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.render_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize action values (q-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.render_q_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.render_q_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
