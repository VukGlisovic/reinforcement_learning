{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi V3 environment\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py\n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Passenger locations:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "- 4: in taxi\n",
    "\n",
    "Destinations:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "\n",
    "Actions:\n",
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east\n",
    "- 3: move west\n",
    "- 4: pickup passenger\n",
    "- 5: drop off passenger\n",
    "\n",
    "Rewards:\n",
    "There is a default per-step reward of -1,\n",
    "except for delivering the passenger, which is +20,\n",
    "or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, agent_type, initial_estimate=0, step_size=0.1, discount=1., epsilon=0.1):\n",
    "        self.q_values = np.ones([env.observation_space.n, env.action_space.n]) * initial_estimate\n",
    "        self.action_space = env.action_space\n",
    "        self.learning_step_fnc = self.get_learning_fnc(agent_type)\n",
    "        self.discount = discount\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "        \n",
    "    def get_learning_fnc(self, agent_type):\n",
    "        if agent_type == 'q-learning':\n",
    "            return self.q_learning_update_step\n",
    "        elif agent_type == 'sarsa':\n",
    "            return self.sarsa_update_step\n",
    "        elif agent_type == 'expected-sarsa':\n",
    "            return self.expected_sarsa_update_step\n",
    "        else:\n",
    "            raise ValueError(\"Agent type '{}' is not supported.\".format(agent_type))\n",
    "        \n",
    "    @staticmethod\n",
    "    def argmax(q_values):\n",
    "        max_value = np.max(q_values)\n",
    "        max_indices = np.where(q_values == max_value)[0]\n",
    "        return np.random.choice(max_indices)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return self.action_space.sample()\n",
    "        return self.argmax(self.q_values[state, :])\n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        self.last_action = self.select_action(state)\n",
    "        self.last_state = state\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        # select next action\n",
    "        action = self.select_action(state)\n",
    "        # update q-value estimates\n",
    "        self.learning_step_fnc(reward, state, action)\n",
    "        # select next action\n",
    "        self.last_action = action\n",
    "        self.last_state = state\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        # update q-value estimates\n",
    "        previous_q_values = self.q_values[self.last_state, :]\n",
    "        td_error = reward - previous_q_values[self.last_action]\n",
    "        self.q_values[self.last_state, self.last_action] += self.step_size * td_error\n",
    "        \n",
    "    def q_learning_update_step(self, reward, state, _):\n",
    "        previous_q_values = self.q_values[self.last_state, :]\n",
    "        current_q_values = self.q_values[state, :]\n",
    "        # q learning update: q <- q + alpha * ((r + discount * q_next) - q)\n",
    "        td_error = reward + self.discount * np.max(current_q_values) - previous_q_values[self.last_action]\n",
    "        self.q_values[self.last_state, self.last_action] += self.step_size * td_error\n",
    "        \n",
    "    def sarsa_update_step(self, reward, state, action):\n",
    "        previous_q_values = self.q_values[self.last_state, :]\n",
    "        current_q_values = self.q_values[state, :]\n",
    "        td_error = reward + self.discount * current_q_values[action] - previous_q_values[self.last_action]\n",
    "        self.q_values[self.last_state, self.last_action] += self.step_size * td_error\n",
    "        \n",
    "    def expected_sarsa_update_step(self, reward, state, action):\n",
    "        previous_q_values = self.q_values[self.last_state, :]\n",
    "        current_q_values = self.q_values[state, :]\n",
    "        # get expected sarsa next value estimate\n",
    "        max_action = self.argmax(current_q_values)\n",
    "        expected_value = 0\n",
    "        for a, q in enumerate(current_q_values):\n",
    "            if a == max_action:\n",
    "                expected_value += (1 - self.epsilon + (self.epsilon / self.action_space.n)) * q\n",
    "            else:\n",
    "                expected_value += (self.epsilon / self.action_space.n) * q\n",
    "        # apply the update\n",
    "        td_error = reward + self.discount * expected_value - previous_q_values[self.last_action]\n",
    "        self.q_values[self.last_state, self.last_action] += self.step_size * td_error\n",
    "\n",
    "    def get_locations(self, env, pass_loc, dest_idx):\n",
    "        \"\"\"\n",
    "        dest_idx: 0 -> (0,0), 1 -> (0,4), 2 -> (4,0), 3 -> (4,3)\n",
    "        pass_loc same as for dest_idx, except that there's also \n",
    "        loc 4 -> 'inside taxi'.\n",
    "        \"\"\"\n",
    "        locs = []\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                locs.append(env.encode(i, j, pass_loc=pass_loc, dest_idx=dest_idx))\n",
    "        return np.array(locs)\n",
    "        \n",
    "    def render(self, env, pass_loc):\n",
    "        dest_loc_map = {0: (0,0), 1: (0,4), 2: (4,0), 3: (4,3)}\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "        dest_indices = range(4)\n",
    "        for ax, dest_idx in zip(np.ravel(axes), dest_indices):\n",
    "            ax.set_title('Destination: {}'.format(dest_loc_map[dest_idx]), fontsize=18)\n",
    "            locs = self.get_locations(env, pass_loc=pass_loc, dest_idx=dest_idx)\n",
    "            sns.heatmap(np.max(self.q_values, axis=1)[locs].reshape((5,5)), cmap='rocket', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(env, agent):\n",
    "    \"\"\"Run one (training) episode.\n",
    "    \"\"\"\n",
    "    observation = env.reset()\n",
    "    agent.agent_start(observation)\n",
    "    terminal = False\n",
    "    cumulative_reward = 0\n",
    "    n_steps = 1\n",
    "    while True:\n",
    "        observation, reward, terminal, info = env.step(agent.last_action)\n",
    "        cumulative_reward += reward\n",
    "        if terminal:\n",
    "            break\n",
    "        agent.agent_step(reward, observation)\n",
    "        n_steps += 1\n",
    "    agent.agent_end(reward)\n",
    "    return cumulative_reward, n_steps\n",
    "\n",
    "\n",
    "def run_experiment(env, agent, n_episodes=2000):\n",
    "    reward_list = []\n",
    "    n_steps_list = []\n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        episode_reward, episode_steps = episode(env, agent)\n",
    "        reward_list.append(episode_reward)\n",
    "        n_steps_list.append(episode_steps)\n",
    "    return reward_list, n_steps_list\n",
    "\n",
    "\n",
    "def animate_episode(agent, interval, title=None):\n",
    "    \"\"\"Animates one episode.\n",
    "    \"\"\"\n",
    "    \n",
    "    def render(title):\n",
    "        if title:\n",
    "            print(title + env.render('ansi'))\n",
    "        else:\n",
    "            env.render('human')\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    location = env.reset()\n",
    "    terminal = False\n",
    "    render(title)\n",
    "    while not terminal:\n",
    "        time.sleep(interval)\n",
    "        location, _, terminal, _ = env.step(agent.select_action(location))\n",
    "        clear_output(wait=True)\n",
    "        render(title)\n",
    "\n",
    "def animate_episodes(agent, n_episodes, interval):\n",
    "    \"\"\"Animates multiple episodes.\n",
    "    \"\"\"\n",
    "    for i in range(1, n_episodes+1):\n",
    "        title = ' episode ' + str(i) + ' \\n'\n",
    "        animate_episode(agent, interval, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_qlearning = Agent(env, 'q-learning', initial_estimate=0, epsilon=0.01, step_size=0.1)\n",
    "# agent_sarsa = Agent(env, 'sarsa', initial_estimate=0, epsilon=0.01)\n",
    "# agent_expected_sarsa = Agent(env, 'expected-sarsa', initial_estimate=0, epsilon=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animate episodes with untrained agent\n",
    "\n",
    "rendering:\n",
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- other letters (R, G, Y and B): locations for passengers and destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animations with random agent\n",
    "animate_episodes(agent_qlearning, 2, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_qlearning, steps_qlearning = run_experiment(env, agent_qlearning)\n",
    "df_results_qlearning = pd.DataFrame(data={'return': rewards_qlearning, 'episode_steps': steps_qlearning})\n",
    "# rewards_sarsa, steps_sarsa = run_experiment(env, agent_sarsa)\n",
    "# rewards_expected_sarsa, steps_expected_sarsa = run_experiment(env, agent_expected_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "ax.set_title(\"Sum of reward per episode\", fontsize=20)\n",
    "ax.set_ylabel('return', fontsize=16)\n",
    "ax.set_xlabel('episode number', fontsize=16)\n",
    "window, min_periods = 50, 10\n",
    "df_results_qlearning['return'].rolling(window=window, min_periods=min_periods).mean().plot(ax=ax, color='green')\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"number of steps\", fontsize=16)\n",
    "df_results_qlearning['episode_steps'].rolling(window=window, min_periods=min_periods).mean().plot(ax=ax2, color='blue')\n",
    "ax.legend(fontsize=14, bbox_to_anchor=(1, 0.55))\n",
    "ax2.legend(fontsize=14, bbox_to_anchor=(1, 0.45));\n",
    "# plt.plot(pd.Series(rewards_sarsa).rolling(window=window).mean());\n",
    "# plt.plot(pd.Series(rewards_expected_sarsa).rolling(window=window).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animate learned agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_episodes(agent_qlearning, 5, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_qlearning.render(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
