{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lunar lander source code: https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from rl_src.animations.video import write_video, show_video, simulate_episode\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rocket trajectory optimization is a classic topic in Optimal Control.\n",
    "\n",
    "According to Pontryagin's maximum principle it's optimal to fire engine full throttle or\n",
    "turn it off. That's the reason this environment is OK to have discreet actions (engine on or off).\n",
    "\n",
    "The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector.\n",
    "Reward for moving from the top of the screen to the landing pad and zero speed is about 100..140 points.\n",
    "If the lander moves away from the landing pad it loses reward. The episode finishes if the lander crashes or\n",
    "comes to rest, receiving an additional -100 or +100 points. Each leg with ground contact is +10 points.\n",
    "Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame.\n",
    "Solved is 200 points.\n",
    "\n",
    "Landing outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land\n",
    "on its first attempt. Please see the source code for details.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_x, pos_y, vel_x, vel_y, lander_angle, lander_angular_velocity, ground contact left leg, ground contact right leg\n",
    "print(env.observation_space)\n",
    "\n",
    "# Nop, fire left engine, main engine, right engine\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# quick experiment to see what the observation space extrema are approximately\n",
    "observations = []\n",
    "for i in range(300):\n",
    "    observation = env.reset()\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        observation, reward, terminal, _ = env.step(env.action_space.sample())\n",
    "        observations.append(observation)\n",
    "\n",
    "data = pd.DataFrame(observations, \n",
    "                    columns=['pos_x', 'pos_y', 'vel_x', 'vel_y', 'lander_angle', 'lander_angular_velocity', 'left_leg', 'right_leg'])\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceList:\n",
    "    \"\"\"Keeps track of past experiences, with a maximum buffer size.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_list_size=10000, batch_size=16):\n",
    "        self.max_list_size = max_list_size\n",
    "        self.batch_size = batch_size\n",
    "        self.experiences = []\n",
    "        \n",
    "    def add_experience(self, state, action, reward, terminal, next_state):\n",
    "        self.experiences.append([state, action, reward, terminal, next_state])\n",
    "        self.experiences = self.experiences[-self.max_list_size:]\n",
    "        \n",
    "    def get_sample(self):\n",
    "        indices = np.random.choice(np.arange(len(self.experiences)), size=self.batch_size, replace=False)\n",
    "        return [self.experiences[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsInit:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def he(shape):\n",
    "        n_out, n_in = shape\n",
    "        return np.random.normal(loc=0, scale=np.sqrt(2/n_in), size=(n_out, n_in))\n",
    "    \n",
    "    @staticmethod\n",
    "    def saxe(shape):\n",
    "        n_out, n_in = shape\n",
    "        weights = np.random.normal(0, 1, shape)\n",
    "        if n_out < n_in:\n",
    "            weights = weights.T\n",
    "        weights, triang = np.linalg.qr(weights)  # orthonormal matrix, upper triangular matrix\n",
    "        diag = np.diag(triang, 0)\n",
    "        diag_sign = np.sign(diag)\n",
    "        weights *= diag_sign\n",
    "        if n_out < n_in:\n",
    "            weights = weights.T\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def gradient_step(self, weights, updates):\n",
    "        for name in weights.keys():\n",
    "            weights[name] += self.learning_rate * updates[name]\n",
    "        return weights\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    \n",
    "    def __init__(self, weights, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize Adam algorithm's m and v\n",
    "        self.first_moment = dict()\n",
    "        self.second_moment = dict()\n",
    "        \n",
    "        for name in weights.keys():\n",
    "            self.first_moment[name] = np.zeros(weights[name].shape)\n",
    "            self.second_moment[name] = np.zeros(weights[name].shape)\n",
    "            \n",
    "        self.beta_1_product = self.beta_1\n",
    "        self.beta_2_product = self.beta_2\n",
    "    \n",
    "    def gradient_step(self, weights, updates):\n",
    "        \"\"\"updates should be based on the gradient multiplied by the td errors.\n",
    "        \"\"\"\n",
    "        for name in weights.keys():\n",
    "            self.first_moment[name] = self.beta_1*self.first_moment[name] + (1-self.beta_1)*updates[name]\n",
    "            self.second_moment[name] = self.beta_2*self.second_moment[name] + (1-self.beta_2)*updates[name]**2\n",
    "            first_moment_hat = self.first_moment[name] / (1 - self.beta_1_product)\n",
    "            second_moment_hat = self.second_moment[name] / (1 - self.beta_2_product)\n",
    "            weight_update = self.learning_rate / (np.sqrt(second_moment_hat) + self.epsilon) * first_moment_hat\n",
    "            weights[name] += weight_update\n",
    "        \n",
    "        self.beta_1_product *= self.beta_1\n",
    "        self.beta_2_product *= self.beta_2\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action-Value neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValueNN:\n",
    "    \n",
    "    def __init__(self, weight_initializer='he', n_features=10, n_hidden_nodes=128, n_outputs=4, gamma=1., tau=1., n_td_steps=0):\n",
    "        self.weight_initializer = weight_initializer\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden_nodes = n_hidden_nodes\n",
    "        self.n_outputs = n_outputs\n",
    "        self.weights = self.initialize_weights(weight_initializer)\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.n_td_steps = n_td_steps\n",
    "        \n",
    "    def initialize_weights(self, weight_initializer):\n",
    "        init_fnc = getattr(WeightsInit, weight_initializer)\n",
    "        weights = dict()\n",
    "        weights['w0'] = init_fnc((self.n_hidden_nodes, self.n_features))\n",
    "        weights['b0'] = np.zeros(shape=(self.n_hidden_nodes, 1))\n",
    "        weights['w1'] = init_fnc((self.n_outputs, self.n_hidden_nodes))\n",
    "        weights['b1'] = np.zeros(shape=(self.n_outputs, 1))\n",
    "        return weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_features(state):\n",
    "        \"\"\"Expected shape of state is (batch_size, n_state_space). state should contain\n",
    "        the following [pos_x, pos_y, vel_x, vel_y, ang, ang_vel, leg_l, leg_r] = state\n",
    "        Adds two features:\n",
    "        - distance from target\n",
    "        - total velocity\n",
    "        \"\"\"\n",
    "        distance = (state[:,0:1]**2 + state[:,1:2]**2)**0.5\n",
    "        velocity = (state[:,2:3]**2 + state[:,3:4]**2)**0.5\n",
    "        return np.concatenate((state, distance, velocity), axis=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(logits, tau=1.):\n",
    "        \"\"\"logits shape: (n_outputs, batch_size)\n",
    "        output shape is the same: (n_outputs, batch_size)\n",
    "        \"\"\"\n",
    "        z = logits - np.max(logits, axis=0, keepdims=True)\n",
    "        z = z / tau\n",
    "        num = np.exp(z)\n",
    "        den = np.sum(num, axis=0)\n",
    "        return num / den\n",
    "    \n",
    "    def forward_propagation(self, features):\n",
    "        \"\"\"features shape: (n_features, batch_size)\n",
    "        output shape: (n_outputs, batch_size)\n",
    "        \"\"\"\n",
    "        z0 = np.dot(self.weights['w0'], features) + self.weights['b0']\n",
    "        a0 = np.maximum(z0, 0)  # ReLU\n",
    "        z1 = np.dot(self.weights['w1'], a0) + self.weights['b1']  # linear output layer\n",
    "        return z1, a0, z0\n",
    "    \n",
    "    def get_action_values(self, features):\n",
    "        action_values, _, _ = self.forward_propagation(features)\n",
    "        return action_values\n",
    "    \n",
    "    def calculate_td_error(self, features, actions, rewards, terminals, next_features, frozen_nn):\n",
    "        # calculate state value estimates of next state from the frozen neural network\n",
    "        next_action_values = frozen_nn.get_action_values(next_features)\n",
    "        next_action_softmaxes = frozen_nn.softmax(next_action_values, self.tau)\n",
    "        next_state_values = np.sum(next_action_softmaxes * next_action_values, axis=0)  # expected sarsa\n",
    "        next_state_values *= (1 - terminals)\n",
    "        # calculate state values estimates of current state\n",
    "        action_values = self.get_action_values(features)\n",
    "        state_values = action_values[actions, np.arange(action_values.shape[1])]  # selected action\n",
    "        # calculate td error\n",
    "        td_error = rewards + self.gamma**self.n_td_steps * next_state_values - state_values  # shape: (batch_size,)\n",
    "        return td_error\n",
    "    \n",
    "    def calculate_td_updates(self, features, td_error_matrix):\n",
    "        \"\"\"Variable shape:\n",
    "        features -> (n_features, batch_size)\n",
    "        td_error_matrix -> (n_actions, batch_size)\n",
    "        z0, a0, d_a0 -> (n_hidden_nodes, batch_size)\n",
    "        z1 -> (n_actions, batch_size)\n",
    "        \"\"\"\n",
    "        batch_size = features.shape[1]\n",
    "        z1, a0, z0 = self.forward_propagation(features)\n",
    "        d_a0 = (a0 > 0).astype('float32')\n",
    "        updates = dict()\n",
    "        updates['w1'] = np.dot(td_error_matrix, a0.T) * (1 / batch_size)  # shape: (n_outputs, n_hidden_nodes)\n",
    "        updates['b1'] = np.sum(td_error_matrix, axis=1, keepdims=True) * (1 / batch_size)  # shape: (n_outputs, 1)\n",
    "        td_error_matrix_backprop = np.dot(self.weights['w1'].T, td_error_matrix) * d_a0  # shape: (n_hidden_nodes, batch_size)\n",
    "        updates['w0'] = np.dot(td_error_matrix_backprop, features.T) * (1 / batch_size)  # shape: (n_hidden_nodes, n_features)\n",
    "        updates['b0'] = np.sum(td_error_matrix_backprop, axis=1, keepdims=True) * (1 / batch_size)  # shape: (n_hidden_nodes, 1)\n",
    "        return updates\n",
    "        \n",
    "    def get_weights_updates(self, experiences, frozen_nn):\n",
    "        batch_size = len(experiences)\n",
    "        states, actions, rewards, terminals, next_states = zip(*experiences)  # unpack data\n",
    "        states = np.array(states)  # shape (batch_size, n_state_space)\n",
    "        features = self.generate_features(states).T  # shape (n_features, batch_size)\n",
    "        next_states = np.array(next_states)  # shape (batch_size, n_state_space)\n",
    "        next_features = self.generate_features(next_states).T  # shape (n_features, batch_size)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        terminals = np.array(terminals)\n",
    "        # calculate td errors\n",
    "        td_error = self.calculate_td_error(features, actions, rewards, terminals, next_features, frozen_nn)\n",
    "        # create td error matrix with non-zero values only for the actions that were actually taken\n",
    "        td_error_matrix = np.zeros((self.n_outputs, batch_size))\n",
    "        td_error_matrix[actions, np.arange(batch_size)] = td_error\n",
    "        # calculate gradients\n",
    "        updates = self.calculate_td_updates(features, td_error_matrix)\n",
    "        return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, agent_setup, exp_list_setup, nn_setup, opt_setup):\n",
    "        self.n_state_space = env.observation_space.shape[0]\n",
    "        self.n_features = self.n_state_space + 2\n",
    "        self.action_space = env.action_space\n",
    "        self.actions = list(range(self.action_space.n))\n",
    "        \n",
    "        self.n_replay_steps = agent_setup.get('n_replay_steps', 4)\n",
    "        self.n_td_steps = agent_setup.get('n_td_steps', 10)\n",
    "        self.freeze_weights = agent_setup.get('freeze_weights', True)\n",
    "        self.experience_list = ExperienceList(**exp_list_setup)\n",
    "        self.nn = ActionValueNN(n_td_steps=self.n_td_steps, n_features=self.n_features, n_outputs=self.action_space.n, **nn_setup)\n",
    "        self.gamma = self.nn.gamma\n",
    "        self.opt = Adam(self.nn.weights, **opt_setup)\n",
    "        \n",
    "        self.reward_history = None\n",
    "        self.state_history = None\n",
    "        self.action_history = None\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        features = self.nn.generate_features(state.reshape((1, self.n_state_space)))\n",
    "        action_logits = self.nn.get_action_values(features=features.T)\n",
    "        softmax_probs = self.nn.softmax(action_logits)\n",
    "        action = np.random.choice(self.actions, p=softmax_probs.reshape(-1))\n",
    "        return action\n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        # initialize first action\n",
    "        action = self.select_action(state)\n",
    "        # initialize history lists\n",
    "        self.reward_history = []\n",
    "        self.state_history = [state]\n",
    "        self.action_history = [action]\n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        # store newly acquired experience\n",
    "        terminal = 0\n",
    "        self.reward_history.append(reward)\n",
    "        # correct length of reward history\n",
    "        self.reward_history = self.reward_history[-self.n_td_steps:]\n",
    "        if len(self.reward_history) >= self.n_td_steps:\n",
    "            reward_sum = sum([self.gamma**i * r for i, r in enumerate(self.reward_history)])\n",
    "            last_state = self.state_history[0]  # we can take the 0th element since we only keep track of the necessary history\n",
    "            last_action = self.action_history[0]\n",
    "            self.experience_list.add_experience(last_state, last_action, reward_sum, terminal, state)\n",
    "        # select next action\n",
    "        action = self.select_action(state)\n",
    "        # learn by replaying experiences\n",
    "        if len(self.experience_list.experiences) >= self.experience_list.batch_size:\n",
    "            if self.freeze_weights:\n",
    "                frozen_nn = deepcopy(self.nn)\n",
    "            else:\n",
    "                frozen_nn = self.nn\n",
    "            for _ in range(self.n_replay_steps):\n",
    "                experience_sample = self.experience_list.get_sample()\n",
    "                updates = self.nn.get_weights_updates(experience_sample, frozen_nn)\n",
    "                self.nn.weights = self.opt.gradient_step(self.nn.weights, updates)\n",
    "            \n",
    "        # save state and action\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(action)\n",
    "        # correct length of state/action history\n",
    "        self.state_history = self.state_history[-self.n_td_steps:]\n",
    "        self.action_history = self.action_history[-self.n_td_steps:]\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        # store newly acquired experience\n",
    "        state = np.zeros(self.state_history[-1].shape)  # set dummy terminal state\n",
    "        terminal = 1\n",
    "        self.reward_history.append(reward)\n",
    "        # correct length of reward history\n",
    "        self.reward_history = self.reward_history[-self.n_td_steps:]\n",
    "        if len(self.reward_history) >= self.n_td_steps:\n",
    "            reward_sum = sum(self.reward_history)\n",
    "            last_state = self.state_history[0]  # we can take the 0th element since we only keep track of the necessary history\n",
    "            last_action = self.action_history[0]\n",
    "            self.experience_list.add_experience(last_state, last_action, reward_sum, terminal, state)\n",
    "        # learn by replaying experiences\n",
    "        if len(self.experience_list.experiences) >= self.experience_list.batch_size:\n",
    "            if self.freeze_weights:\n",
    "                frozen_nn = deepcopy(self.nn)\n",
    "            else:\n",
    "                frozen_nn = self.nn\n",
    "            for _ in range(self.n_replay_steps):\n",
    "                experience_sample = self.experience_list.get_sample()\n",
    "                updates = self.nn.get_weights_updates(experience_sample, frozen_nn)\n",
    "                self.nn.weights = self.opt.gradient_step(self.nn.weights, updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(path, obj):\n",
    "    if '/' in path:\n",
    "        directory, fname = path.rsplit('/', 1)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    pickle.dump(obj, open(path, \"wb\"))\n",
    "\n",
    "\n",
    "def load_object(path):\n",
    "    return pickle.load(open(path, \"rb\"))\n",
    "\n",
    "\n",
    "def episode(env, agent):\n",
    "    \"\"\"Run one (training) episode.\n",
    "    \"\"\"\n",
    "    last_observation = env.reset()\n",
    "    terminal = False\n",
    "    cumulative_reward = 0\n",
    "    episode_n_steps = 0\n",
    "    agent.agent_start(last_observation)\n",
    "    while not terminal:\n",
    "        observation, reward, terminal, info = env.step(agent.action_history[-1])\n",
    "        agent.agent_step(reward, observation)\n",
    "        cumulative_reward += reward\n",
    "        episode_n_steps += 1\n",
    "    agent.agent_end(reward)\n",
    "    return cumulative_reward, episode_n_steps\n",
    "\n",
    "\n",
    "def run_experiment(env, agent, n_episodes=100, checkpoint_freq=None, checkpoint_path_template=None, results_output_path=None, df_prev_results=None):\n",
    "    reward_list = []\n",
    "    episode_n_steps_list = []\n",
    "    ep_start = 1\n",
    "    if df_prev_results is not None:\n",
    "        # if previous results given, then this means continue training from certain point\n",
    "        reward_list = df_prev_results['reward'].tolist()\n",
    "        episode_n_steps_list = df_prev_results['episode_steps'].tolist()\n",
    "        ep_start = len(df_prev_results) + 1\n",
    "        print(\"Continuing from older results from episode {}\".format(len(df_prev_results)))\n",
    "    \n",
    "    for i in tqdm(range(ep_start, ep_start + n_episodes)):\n",
    "        episode_reward, episode_n_steps = episode(env, agent)\n",
    "        reward_list.append(episode_reward)\n",
    "        episode_n_steps_list.append(episode_n_steps)\n",
    "        if checkpoint_freq is not None and (i % checkpoint_freq) == 0:\n",
    "            checkpoint_path = checkpoint_path_template.format(i)\n",
    "            save_object(checkpoint_path, agent)\n",
    "            df_results = pd.DataFrame(data={'reward': reward_list, 'episode_steps': episode_n_steps_list})\n",
    "            if results_output_path is not None:\n",
    "                df_results.to_csv(results_output_path, index=False)\n",
    "            \n",
    "    df_results = pd.DataFrame(data={'reward': reward_list, 'episode_steps': episode_n_steps_list})\n",
    "    if results_output_path is not None:\n",
    "        df_results.to_csv(results_output_path, index=False)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "def animate_episode(env, agent):\n",
    "    if agent == 'random':\n",
    "        action_fnc = lambda x: env.action_space.sample()\n",
    "    else:\n",
    "        action_fnc = lambda x: agent.select_action(x)\n",
    "    obs = env.reset()\n",
    "    terminal = False\n",
    "    i = 0\n",
    "    while not terminal and i < 500:\n",
    "        obs, _, terminal, _ = env.step(action_fnc(obs))\n",
    "        env.render()\n",
    "        i += 1\n",
    "        time.sleep(1/30)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random agent\n",
    "simulate_episode(env, lambda x: env.action_space.sample(), width=600, play_type='autoplay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_output_path = 'agents/train_results.csv'\n",
    "checkpoint_path_template = 'agents/lunarlander_ep{:04}.pickle'\n",
    "checkpoint_frequency = 100\n",
    "n_episodes = 1000\n",
    "config = {\n",
    "    'agent_setup': {'n_td_steps': 10, 'n_replay_steps': 4, 'freeze_weights': True},\n",
    "    'exp_list_setup': {'max_list_size': 50000, 'batch_size': 8},\n",
    "    'nn_setup': {'weight_initializer': 'saxe', 'n_hidden_nodes': 256, 'gamma': 0.99, 'tau': 0.001},\n",
    "    'opt_setup': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08}\n",
    "}\n",
    "\n",
    "\n",
    "agent = Agent(env, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_experiment(\n",
    "    env, \n",
    "    agent, \n",
    "    n_episodes=n_episodes, \n",
    "    checkpoint_freq=checkpoint_frequency, \n",
    "    checkpoint_path_template=checkpoint_path_template,\n",
    "    results_output_path=results_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(results_output_path, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(15,5))\n",
    "\n",
    "window = 100\n",
    "min_periods = 10\n",
    "rol_rewards = results['reward'].rolling(window=window, min_periods=min_periods).mean()\n",
    "rol_rewards.plot(color='green', alpha=0.8, ax=ax1)\n",
    "ax1.vlines(range(0, len(results)+1, 100), rol_rewards.min(), rol_rewards.max(), color='black', lw=0.5, ls='--', alpha=0.5)\n",
    "ax1.set_xlabel('episode number', fontsize=16)\n",
    "ax1.set_ylabel('cumulative reward', fontsize=16)\n",
    "ax2 = ax1.twinx()\n",
    "results['episode_steps'].rolling(window=window, min_periods=min_periods).mean().plot(color='blue', alpha=0.8, ax=ax2)\n",
    "ax2.set_ylabel('number of steps', fontsize=16)\n",
    "ax1.legend(loc=2, fontsize=14)\n",
    "ax2.legend(loc=4, fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_object(checkpoint_path_template.format(n_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained agent\n",
    "simulate_episode(env, agent.select_action, width=800, play_type='controls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
