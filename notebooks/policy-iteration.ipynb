{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment (GridWorld) class\n",
    "\n",
    "* Number of states is based on width and height of the grid of the environment (n_states = width * height)\n",
    "* Every state has 4 actions; move west, east, north or south\n",
    "* Goal is to find the exit (one of the terminal states)\n",
    "* Reward is -1 for each step; this encourages the agent to find the exit as soon as possible\n",
    "* The discount factor we'll set to 1; meaning a farsighted agent\n",
    "* If an action takes the agent off the grid, then the agent is put back into the same state it executed that action from\n",
    "\n",
    "The state-values will be initialized to zero and the agents' policy will be initialized to a random policy meaning each action has equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"Creates a grid of size width x height. terminal_states is\n",
    "    a list of integers that indicate what the terminal states are. \n",
    "    Each step in the gridworld, yields a reward of -1; this should\n",
    "    give incentive to the agent to find the exit as fast as possible.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width, height, terminal_states):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.n_states = self.width * self.height\n",
    "        self.states = list(range(self.n_states))\n",
    "        self.v_values = [0] * self.n_states\n",
    "        self.terminal_states = terminal_states\n",
    "        self.validate_terminal_states()\n",
    "    \n",
    "    def validate_terminal_states(self):\n",
    "        assert isinstance(self.terminal_states, list), \"terminal_states must be a list.\"\n",
    "        for state in self.terminal_states:\n",
    "            assert (state in self.states), \"Terminal state {} not in state set.\".format(state)\n",
    "            \n",
    "    def state_to_coordinate(self, state):\n",
    "        xloc = state % self.width\n",
    "        yloc = int(state / self.width)\n",
    "        return (xloc, yloc)\n",
    "            \n",
    "    def coordinate_to_state(self, xloc, yloc):\n",
    "        return yloc * self.width + xloc\n",
    "    \n",
    "    def is_valid_coordinate(self, xloc, yloc):\n",
    "        if xloc < 0 or xloc >= self.width:\n",
    "            return False\n",
    "        if yloc < 0 or yloc >= self.height:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        if action == 0:\n",
    "            xloc, yloc = self.state_to_coordinate(state)\n",
    "            xloc -= 1\n",
    "            if not self.is_valid_coordinate(xloc, yloc):\n",
    "                xloc += 1  # revert change\n",
    "        elif action == 1:\n",
    "            xloc, yloc = self.state_to_coordinate(state)\n",
    "            xloc += 1\n",
    "            if not self.is_valid_coordinate(xloc, yloc):\n",
    "                xloc -= 1  # revert change\n",
    "        elif action == 2:\n",
    "            xloc, yloc = self.state_to_coordinate(state)\n",
    "            yloc -= 1\n",
    "            if not self.is_valid_coordinate(xloc, yloc):\n",
    "                yloc += 1  # revert change\n",
    "        elif action == 3:\n",
    "            xloc, yloc = self.state_to_coordinate(state)\n",
    "            yloc += 1\n",
    "            if not self.is_valid_coordinate(xloc, yloc):\n",
    "                yloc -= 1  # revert change\n",
    "        observation = self.coordinate_to_state(xloc, yloc)\n",
    "        reward, terminal, info = -1, False, dict()\n",
    "        if observation in self.terminal_states:\n",
    "            terminal = True\n",
    "        return observation, reward, terminal, info\n",
    "    \n",
    "    def policy_evaluation_sweep(self, policy):\n",
    "        new_v_values = self.v_values.copy()\n",
    "        for state in self.states:\n",
    "            if state in self.terminal_states:\n",
    "                # state-value of terminal state always stays zero\n",
    "                continue\n",
    "            state_policy = policy[state]\n",
    "            new_v = 0\n",
    "            for action, pr_action in enumerate(state_policy):\n",
    "                state_prime, reward, _, _ = self.step(state, action)\n",
    "                new_v += pr_action * 1. * (reward + self.v_values[state_prime])\n",
    "            new_v_values[state] = new_v\n",
    "        self.v_values = new_v_values\n",
    "        \n",
    "    def render_v_values(self, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(self.width, self.height))\n",
    "        data = np.reshape(self.v_values, (self.height, self.width))\n",
    "        sns.heatmap(data, cmap='coolwarm', annot=data, fmt='.3g', annot_kws={'fontsize': 14}, cbar=False, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Based on what the environment looks like, creates an agent\n",
    "    that can do four actions: move west, east, north or south. The\n",
    "    policy is that each of the actions will be selected with equal\n",
    "    probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width, height, terminal_states):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.n_states = self.width * self.height\n",
    "        self.policy = np.ones((self.n_states, 4)) * 0.25\n",
    "        self.terminal_states = terminal_states\n",
    "        \n",
    "    def policy_improvement(self, env):\n",
    "        for state in range(self.policy.shape[0]):\n",
    "            if state in self.terminal_states:\n",
    "                # no policy needed in terminal state\n",
    "                continue\n",
    "            state_policy = self.policy[state]\n",
    "            action_values = np.zeros(len(state_policy))\n",
    "            for action, pr_action in enumerate(state_policy):\n",
    "                state_prime, reward, _, _ = env.step(state, action)\n",
    "                action_values[action] = (1. * (reward + round(env.v_values[state_prime], 5)))  # round for numerical imprecision\n",
    "            top_actions = np.where(action_values == max(action_values))[0]\n",
    "            pr_top_actions = 1 / len(top_actions)\n",
    "            new_state_policy = np.zeros(len(state_policy))\n",
    "            new_state_policy[top_actions] = pr_top_actions\n",
    "            self.policy[state, :] = new_state_policy\n",
    "\n",
    "    def render_policy(self, env, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(self.width, self.height))\n",
    "\n",
    "        ax.hlines(range(self.height+1), 0, self.width, color='black', lw=1)\n",
    "        ax.vlines(range(self.width+1), 0, self.height, color='black', lw=1)\n",
    "        ax.set_xlim(0, self.width)\n",
    "        ax.set_ylim(0, self.height)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        for state in range(self.policy.shape[0]):\n",
    "            xloc, yloc = env.state_to_coordinate(state)\n",
    "            if state in self.terminal_states:\n",
    "                rect = Rectangle((xloc, yloc), width=1, height=1, ec='black', fc='black', alpha=0.3)\n",
    "                ax.add_patch(rect)\n",
    "                continue\n",
    "            xloc += 0.5\n",
    "            yloc += 0.5\n",
    "            if self.policy[state, 0]:\n",
    "                ax.arrow(xloc, yloc, dy=0, dx=-0.3, head_width=0.09, fc='black', length_includes_head=True)\n",
    "            if self.policy[state, 1]:\n",
    "                ax.arrow(xloc, yloc, dy=0, dx=0.3, head_width=0.09, fc='black', length_includes_head=True)\n",
    "            if self.policy[state, 2]:\n",
    "                ax.arrow(xloc, yloc, dy=-0.3, dx=0, head_width=0.09, fc='black', length_includes_head=True)\n",
    "            if self.policy[state, 3]:\n",
    "                ax.arrow(xloc, yloc, dy=0.3, dx=0, head_width=0.09, fc='black', length_includes_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example GridWorld\n",
    "* width=7\n",
    "* height=3\n",
    "* terminal_states=[(0,0), (0,5), (2,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example GridWorld with example agent\n",
    "width, height = 7, 3\n",
    "terminal_states = [0, 5, 20]\n",
    "example_env = GridWorld(width, height, terminal_states)\n",
    "example_env.render_v_values()\n",
    "Agent(width, height, terminal_states).render_policy(example_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simple 4x4 GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration_illustration(env, agent, show_iterations):\n",
    "    \"\"\"Runs policy evaluation with respect to the random policy.\n",
    "    In addition runs policy improvement based on the state-value\n",
    "    estimates of the random policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def plot(iteration, ax1_title=None, ax2_title=None):\n",
    "        fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(9, 4))\n",
    "        if ax1_title:\n",
    "            ax1.text(0.5, 1.2, ax1_title, fontsize=18, ha='center', va='center', transform=ax1.transAxes)\n",
    "        if ax2_title:\n",
    "            ax2.text(0.5, 1.2, ax2_title, fontsize=18, ha='center', va='center', transform=ax2.transAxes)\n",
    "        env.render_v_values(ax1)\n",
    "        agent.render_policy(env, ax2)\n",
    "        ax2.text(1.1, 0.5, \"$k={}$\".format(iteration), fontsize=22, ha='left', va='center', transform=ax2.transAxes)\n",
    "    \n",
    "    plot(0, \"$v_{k}$ for the random policy\", \"Greedy policy w.r.t. $v_{k}$\")\n",
    "    random_policy = agent.policy.copy()\n",
    "    for i in range(1, max(show_iterations)+1):\n",
    "        env.policy_evaluation_sweep(random_policy)  # update estimate of state-value function with respect to the random policy\n",
    "        agent.policy_improvement(env)  # improve policy greedily with respect to latest state-value function\n",
    "        if i in show_iterations:\n",
    "            plot(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 4, 4\n",
    "terminal_states = [0, 15]\n",
    "env = GridWorld(width, height, terminal_states)\n",
    "agent = Agent(width, height, terminal_states)\n",
    "policy_iteration_illustration(env, agent, [0,1,2,3,10,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we do one step of policy evaluation?\n",
    "\n",
    "Note the following:\n",
    "* the policy is the random policy: $\\pi(a|s) = 0.25$ for all $a \\in \\{\\text{west, east, north, south}\\}$\n",
    "* remember that $\\gamma = 1$\n",
    "* the environment dynamics are deterministic, meaning that for each action, we know exactly what the next state will be.<br/>\n",
    "* Recall the Bellman update equation:\n",
    "$$\n",
    "v_{k+1}(s) \n",
    "= \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s',r | s,a) [r + \\gamma v_{k}(s')]\n",
    "$$\n",
    "\n",
    "\n",
    "Now let's say we want to update $v_{200}(s)$ in coordinate (2,1), then, uing the above equation, we get:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v_{201}((2,1)) = & \n",
    "\\pi(\\text{west}|(2,1)) * \\sum_{s', r} p(s',r | (2,1),\\text{west}) [r + 1 * v_{200}(s')] \\\\&\n",
    "+ \\pi(\\text{east}|(2,1)) * \\sum_{s', r} p(s',r | (2,1),\\text{east}) [r + 1 * v_{200}(s')] \\\\&\n",
    "+ \\pi(\\text{north}|(2,1)) * \\sum_{s', r} p(s',r | (2,1),\\text{north}) [r + 1 * v_{200}(s')] \\\\&\n",
    "+ \\pi(\\text{south}|(2,1)) * \\sum_{s', r} p(s',r | (2,1),\\text{south}) [r + 1 * v_{200}(s')] \\\\&\n",
    "= \\pi(\\text{west}|(2,1)) * p((2,0),-1 | (2,1),\\text{west}) [-1 + 1 * v_{200}((2,0))] \\\\&\n",
    "+ \\pi(\\text{east}|(2,1)) * p((2,2),-1 | (2,1),\\text{east}) [-1 + 1 * v_{200}((2,2))] \\\\&\n",
    "+ \\pi(\\text{north}|(2,1)) * p((1,1),-1 | (2,1),\\text{north}) [-1 + 1 * v_{200}((1,1))] \\\\&\n",
    "+ \\pi(\\text{south}|(2,1)) * p((3,1),-1 | (2,1),\\text{south}) [-1 + 1 * v_{200}((3,1))] \\\\&\n",
    "= (0.25 * 1 * (-1 -20)) + (0.25 * 1 * (-1 -18)) + (0.25 * 1 * (-1 -18)) + (0.25 * 1 * (-1 -20)) \\\\&\n",
    "= -20\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigger 10x10 GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 10, 10\n",
    "terminal_states = [27]\n",
    "env = GridWorld(width, height, terminal_states)\n",
    "agent = Agent(width, height, terminal_states)\n",
    "env.render_v_values()\n",
    "agent.render_policy(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy evaluation for 1000 steps (estimate $v_{\\pi_{0}}$) and 1 policy improvement step (estimate $\\pi_{1}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    env.policy_evaluation_sweep(agent.policy)\n",
    "env.render_v_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_improvement(env)\n",
    "agent.render_policy(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy evaluation for 1000 steps with $\\pi_{1}$ (estimate $v_{\\pi_{1}}$) and again one policy improvement step (estimate $\\pi_{2}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    env.policy_evaluation_sweep(agent.policy)\n",
    "env.render_v_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_improvement(env)\n",
    "agent.render_policy(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus after 2 policy evaluation and 2 policy improvement steps, we converged to our final (optimal solution).\n",
    "\n",
    "$$\n",
    "\\pi_{0} \n",
    "\\xrightarrow{\\text{E}} v_{\\pi_{0}}\n",
    "\\xrightarrow{\\text{I}} \\pi_{1}\n",
    "\\xrightarrow{\\text{E}} v_{\\pi_{1}}\n",
    "\\xrightarrow{\\text{I}} \\pi_{2}\n",
    "$$\n",
    "\n",
    "where $v_{\\pi_{1}} = v_{\\pi_{*}}$ and $\\pi_{2} = \\pi_{*}$.\n",
    "\n",
    "Officially, we'd have to do one more policy evaluation step to obtain $v_{\\pi_{2}}$, but we know and can see that this will be equal to $v_{\\pi_{1}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
