{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mountain car source code: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import rl.features.tile_coding as tc\n",
    "from rl.animations.video import write_video, show_video, simulate_episode\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Observation:\n",
    "    Type: Box(2)\n",
    "    Num    Observation               Min            Max\n",
    "    0      Car Position              -1.2           0.6\n",
    "    1      Car Velocity              -0.07          0.07\n",
    "    \n",
    "Actions:\n",
    "    Type: Discrete(3)\n",
    "    Num    Action\n",
    "    0      Accelerate to the Left\n",
    "    1      Don't accelerate\n",
    "    2      Accelerate to the Right\n",
    "    Note: This does not affect the amount of velocity affected by the\n",
    "    gravitational pull acting on the car.\n",
    "    \n",
    "Reward:\n",
    "     Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
    "     on top of the mountain.\n",
    "     Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "     \n",
    "Starting State:\n",
    "     The position of the car is assigned a uniform random value in\n",
    "     [-0.6 , -0.4].\n",
    "     The starting velocity of the car is always assigned to 0.\n",
    "     \n",
    "Episode Termination:\n",
    "     The car position is more than 0.5\n",
    "     Episode length is greater than 200\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\").env\n",
    "setattr(env, 'x_bottom', -0.5236)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, step_size=0.01, epsilon=0.1, gamma=1, num_tilings=8, num_tiles=8, iht_size=4096):\n",
    "        self.action_space = env.action_space\n",
    "        self.gamma = gamma\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_action_value = None\n",
    "        self.last_state = None\n",
    "        self.last_features = None\n",
    "        \n",
    "        self.iht_size = iht_size\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        \n",
    "        self.action_weights = [np.zeros(self.iht_size) for i in range(self.action_space.n)]\n",
    "    \n",
    "    def state_to_tiles(self, state):\n",
    "        position, velocity = state\n",
    "        position_scaled = ((position + 1.2) / 1.7) * self.num_tiles\n",
    "        velocity_scaled = ((velocity + 0.07) / 0.14) * self.num_tiles\n",
    "        return np.array(tc.tiles(self.iht, self.num_tilings, [position_scaled, velocity_scaled]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def argmax(q_values):\n",
    "        max_value = np.max(q_values) - 1e-3\n",
    "        max_indices = np.where(q_values >= max_value)[0]\n",
    "        return np.random.choice(max_indices)\n",
    "    \n",
    "    def select_action(self, features):\n",
    "        action_values = self.get_all_action_values(features)\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            action = self.argmax(action_values)\n",
    "        return action, action_values[action]\n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        features = self.state_to_tiles(state)\n",
    "        self.last_action, self.last_action_value = self.select_action(features)\n",
    "        self.last_state = state\n",
    "        self.last_features = features\n",
    "        return self.last_action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        features = self.state_to_tiles(state)\n",
    "        action, action_value = self.select_action(features)\n",
    "        # update weights\n",
    "        td_error = reward + self.gamma * action_value - self.last_action_value\n",
    "        self.action_weights[self.last_action][self.last_features] += self.step_size * td_error  # *1 for the gradient officially\n",
    "        # select next action\n",
    "        self.last_action = action\n",
    "        self.last_action_value = action_value\n",
    "        self.last_state = state\n",
    "        self.last_features = features\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        # update weights\n",
    "        action_value = 0\n",
    "        td_error = reward + self.gamma * action_value - self.last_action_value\n",
    "        self.action_weights[self.last_action][self.last_features] += self.step_size * td_error  # *1 for the gradient officially\n",
    "    \n",
    "    def get_action_value(self, features, action):\n",
    "        return sum(self.action_weights[action][features])\n",
    "    \n",
    "    def get_all_action_values(self, features):\n",
    "        action_values = []\n",
    "        for a in range(self.action_space.n):\n",
    "            action_values.append(self.get_action_value(features, a))\n",
    "        return action_values\n",
    "    \n",
    "    def render_visitations(self, env, observations_arr):\n",
    "        # prepare pandas data frame\n",
    "        states = pd.DataFrame(data=observations_arr, columns=['position', 'velocity'])\n",
    "        position_linspace = np.linspace(-1.2, 0.6, 181)\n",
    "        velocity_linspace = np.linspace(-0.07, 0.07, 141)\n",
    "        states['position_bin'] = pd.IntervalIndex(pd.cut(states['position'], position_linspace)).mid\n",
    "        states['velocity_bin'] = pd.IntervalIndex(pd.cut(states['velocity'], velocity_linspace)).mid\n",
    "        state_visitation_counts = states.groupby(['position_bin', 'velocity_bin'])['position'].count().unstack(fill_value=0)\n",
    "        state_visitation_counts.index = np.round(state_visitation_counts.index, 3)\n",
    "        state_visitation_counts.columns = np.round(state_visitation_counts.columns, 3)\n",
    "        # create heatmap of visitation count\n",
    "        fig, ax = plt.subplots(figsize=(18, 9))\n",
    "        sns.heatmap(np.log(state_visitation_counts + 1).T.sort_index(ascending=False), ax=ax)\n",
    "        ax.set_title(\"Natural log state visitation counts\", fontsize=20)\n",
    "        ax.set_xlabel(\"position\", fontsize=16)\n",
    "        ax.set_ylabel(\"velocity\", fontsize=16)\n",
    "        x_scaled = (env.x_bottom - state_visitation_counts.index[0]) / (state_visitation_counts.index[-1] - state_visitation_counts.index[0])\n",
    "        x_scaled *= (ax.get_xlim()[1] - ax.get_xlim()[0])\n",
    "        ax.axvline(x_scaled, color='white', lw=1)\n",
    "        ax.text(x_scaled+1, ax.get_ylim()[0]-1, \"bottom of valley\", fontsize=14, color='white', ha='center', va='bottom')\n",
    "        \n",
    "    def render_state_values(self):\n",
    "        # create data frames with state values and top action\n",
    "        position_linspace = np.linspace(-1.2, 0.6, 181)\n",
    "        velocity_linspace = np.linspace(-0.07, 0.07, 141)\n",
    "        df_state_values = pd.DataFrame(index=position_linspace, columns=velocity_linspace, dtype='float32')\n",
    "        df_top_action = pd.DataFrame(index=position_linspace, columns=velocity_linspace)\n",
    "        for i in position_linspace:\n",
    "            for j in velocity_linspace:\n",
    "                features = self.state_to_tiles(np.array([i,j]))\n",
    "                action_values = self.get_all_action_values(features)\n",
    "                top_action = self.argmax(action_values)\n",
    "                df_top_action.loc[i, j] = top_action\n",
    "                df_state_values.loc[i, j] = action_values[top_action]\n",
    "        df_top_action.index = np.round(df_top_action.index, 3)\n",
    "        df_top_action.columns = np.round(df_top_action.columns, 3)\n",
    "        df_state_values.index = np.round(df_state_values.index, 3)\n",
    "        df_state_values.columns = np.round(df_state_values.columns, 3)\n",
    "        # create heatmaps of state values and top action\n",
    "        fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(18, 18))\n",
    "        sns.heatmap(df_state_values.T.sort_index(ascending=False), ax=ax1)\n",
    "        ax1.set_title(\"State value estimates\", fontsize=20)\n",
    "        sns.heatmap(df_top_action.astype(int).T.sort_index(ascending=False), ax=ax2)\n",
    "        ax2.set_title(\"Action from top action value\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(env, agent):\n",
    "    \"\"\"Run one (training) episode.\n",
    "    \"\"\"\n",
    "    step_nr = 0\n",
    "    last_observation = env.reset()\n",
    "    terminal = False\n",
    "    cumulative_reward = 0\n",
    "    state_list = []\n",
    "    agent.agent_start(last_observation.reshape((2,1)))\n",
    "    while not terminal and step_nr < 300:\n",
    "        observation, reward, terminal, info = env.step(agent.last_action)\n",
    "        agent.agent_step(reward, observation.reshape((2,1)))\n",
    "        cumulative_reward += reward\n",
    "        state_list.append(observation)\n",
    "        step_nr += 1\n",
    "    agent.agent_end(reward)\n",
    "    return cumulative_reward, state_list\n",
    "\n",
    "\n",
    "def run_experiment(env, agent):\n",
    "    reward_list = []\n",
    "    state_lists = []\n",
    "    for i in tqdm(range(1000)):\n",
    "        episode_reward, state_list = episode(env, agent)\n",
    "        reward_list.append(episode_reward)\n",
    "        state_lists.append(state_list)\n",
    "    return reward_list, state_lists\n",
    "\n",
    "\n",
    "def animate_episode(env, agent):\n",
    "    if agent == 'random':\n",
    "        action_fnc = lambda x: env.action_space.sample()\n",
    "    elif agent == 'right':\n",
    "        action_fnc = lambda x: 2\n",
    "    else:\n",
    "        action_fnc = lambda x: agent.select_action(agent.state_to_tiles(x))[0]\n",
    "    obs = env.reset()\n",
    "    terminal = False\n",
    "    i = 0\n",
    "    while not terminal and i < 500:\n",
    "        obs, _, terminal, _ = env.step(action_fnc(obs))\n",
    "        env.render()\n",
    "        i += 1\n",
    "        time.sleep(1/30)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random agent\n",
    "simulate_episode(env, lambda x: env.action_space.sample(), max_steps=250, width=500, play_type='autoplay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent that only goes right\n",
    "simulate_episode(env, lambda x: 2, max_steps=300, width=500, play_type='autoplay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, step_size=0.01, epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, observations = run_experiment(env, agent)\n",
    "observations = np.array([s for lst in observations for s in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "plt.plot(pd.Series(rewards).rolling(window=20).mean())\n",
    "ax.set_title(\"Rolling mean of total reward per episode\", fontsize=20)\n",
    "ax.set_xlabel(\"Episode Number\", fontsize=16)\n",
    "ax.set_ylabel(\"Reward per Episode\", fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained agent\n",
    "simulate_episode(env, lambda x: agent.select_action(agent.state_to_tiles(x))[0], max_steps=300, width=500, play_type='autoplay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.render_visitations(env, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.render_state_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
