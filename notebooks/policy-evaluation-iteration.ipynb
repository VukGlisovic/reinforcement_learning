{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment (GridWorld) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"Creates a grid of size width x height. terminal_states is\n",
    "    a list of integers that indicate what the terminal states are. \n",
    "    Each step in the gridworld, yields a reward of -1; this should\n",
    "    give incentive to the agent to find the exit as fast as possible.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width, height, terminal_states):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.n_states = self.width * self.height\n",
    "        self.states = list(range(self.n_states))\n",
    "        self.v_values = [0] * self.n_states\n",
    "        self.terminal_states = terminal_states\n",
    "        self.validate_terminal_states()\n",
    "    \n",
    "    def validate_terminal_states(self):\n",
    "        assert isinstance(self.terminal_states, list), \"terminal_states must be a list.\"\n",
    "        for state in self.terminal_states:\n",
    "            assert (state in self.states), \"Terminal state {} not in state set.\".format(state)\n",
    "            \n",
    "    def state_to_coordinate(self, state):\n",
    "        xloc = state % self.width\n",
    "        yloc = int(state / self.width)\n",
    "        return (xloc, yloc)\n",
    "            \n",
    "    def coordinate_to_state(self, xloc, yloc):\n",
    "        return yloc * self.width + xloc\n",
    "    \n",
    "    def is_valid_coordinate(self, xloc, yloc):\n",
    "        if xloc < 0 or xloc >= self.width:\n",
    "            return False\n",
    "        if yloc < 0 or yloc >= self.height:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        if action == 0:\n",
    "            xloc, yloc = self.state_to_coordinate(state)\n",
    "            xloc -= 1\n",
    "            if not self.is_valid_coordinate(xloc, yloc):\n",
    "                xloc += 1  # revert change\n",
    "        elif action == 1:\n",
    "            xloc, yloc = self.state_to_coordinate(state)\n",
    "            xloc += 1\n",
    "            if not self.is_valid_coordinate(xloc, yloc):\n",
    "                xloc -= 1  # revert change\n",
    "        elif action == 2:\n",
    "            xloc, yloc = self.state_to_coordinate(state)\n",
    "            yloc -= 1\n",
    "            if not self.is_valid_coordinate(xloc, yloc):\n",
    "                yloc += 1  # revert change\n",
    "        elif action == 3:\n",
    "            xloc, yloc = self.state_to_coordinate(state)\n",
    "            yloc += 1\n",
    "            if not self.is_valid_coordinate(xloc, yloc):\n",
    "                yloc -= 1  # revert change\n",
    "        observation = self.coordinate_to_state(xloc, yloc)\n",
    "        reward, terminal, info = -1, False, dict()\n",
    "        if observation in self.terminal_states:\n",
    "            terminal = True\n",
    "        return observation, reward, terminal, info\n",
    "    \n",
    "    def policy_evaluation_sweep(self, policy):\n",
    "        new_v_values = self.v_values.copy()\n",
    "        for state in self.states:\n",
    "            if state in self.terminal_states:\n",
    "                # state-value of terminal state always stays zero\n",
    "                continue\n",
    "            state_policy = policy[state]\n",
    "            new_v = 0\n",
    "            for action, pr_action in enumerate(state_policy):\n",
    "                state_prime, reward, _, _ = self.step(state, action)\n",
    "                new_v += pr_action * 1. * (reward + self.v_values[state_prime])\n",
    "            new_v_values[state] = new_v\n",
    "        self.v_values = new_v_values\n",
    "        \n",
    "    def render_v_values(self):\n",
    "        fig, ax = plt.subplots(figsize=(self.width, self.height))\n",
    "        data = np.reshape(self.v_values, (self.height, self.width))\n",
    "        sns.heatmap(data, cmap='coolwarm', annot=data, fmt='.3g', annot_kws={'fontsize': 14}, cbar=False, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Based on what the environment looks like, creates an agent\n",
    "    that can do four actions: move west, east, north or south. The\n",
    "    policy is that each of the actions will be selected with equal\n",
    "    probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width, height, terminal_states):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.n_states = self.width * self.height\n",
    "        self.policy = np.ones((self.n_states, 4)) * 0.25\n",
    "        self.terminal_states = terminal_states\n",
    "        \n",
    "    def policy_improvement(self, env):\n",
    "        for state in range(self.policy.shape[0]):\n",
    "            if state in self.terminal_states:\n",
    "                # no policy needed in terminal state\n",
    "                continue\n",
    "            state_policy = self.policy[state]\n",
    "            state_value = env.v_values[state]\n",
    "            action_values = np.zeros(len(state_policy))\n",
    "            for action, pr_action in enumerate(state_policy):\n",
    "                state_prime, reward, _, _ = env.step(state, action)\n",
    "                action_values[action] = (1. * (reward + env.v_values[state_prime]))\n",
    "            top_actions = np.where(action_values == max(action_values))[0]\n",
    "            pr_top_actions = 1 / len(top_actions)\n",
    "            new_state_policy = np.zeros(len(state_policy))\n",
    "            new_state_policy[top_actions] = pr_top_actions\n",
    "            self.policy[state, :] = new_state_policy\n",
    "\n",
    "    def render_policy(self):\n",
    "        fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "        ax.hlines(range(height+1), 0, width, color='black', lw=1)\n",
    "        ax.vlines(range(width+1), 0, height, color='black', lw=1)\n",
    "        ax.set_xlim(0, width)\n",
    "        ax.set_ylim(0, height)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        for state in range(self.policy.shape[0]):\n",
    "            xloc, yloc = env.state_to_coordinate(state)\n",
    "            if state in self.terminal_states:\n",
    "                rect = Rectangle((xloc, yloc), width=1, height=1, ec='black', fc='black', alpha=0.3)\n",
    "                ax.add_patch(rect)\n",
    "                continue\n",
    "            xloc += 0.5\n",
    "            yloc += 0.5\n",
    "            if self.policy[state, 0]:\n",
    "                ax.arrow(xloc, yloc, dy=0, dx=-0.3, head_width=0.09, fc='black', length_includes_head=True)\n",
    "            if self.policy[state, 1]:\n",
    "                ax.arrow(xloc, yloc, dy=0, dx=0.3, head_width=0.09, fc='black', length_includes_head=True)\n",
    "            if self.policy[state, 2]:\n",
    "                ax.arrow(xloc, yloc, dy=-0.3, dx=0, head_width=0.09, fc='black', length_includes_head=True)\n",
    "            if self.policy[state, 3]:\n",
    "                ax.arrow(xloc, yloc, dy=0.3, dx=0, head_width=0.09, fc='black', length_includes_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simple 4x4 GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 4, 4\n",
    "terminal_states = [0, 15]\n",
    "env = GridWorld(width, height, terminal_states)\n",
    "agent = Agent(width, height, terminal_states)\n",
    "agent.render_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render_v_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.policy_evaluation_sweep(agent.policy)\n",
    "env.render_v_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_improvement(env)\n",
    "agent.render_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigger 10x10 GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 10, 10\n",
    "terminal_states = [27]\n",
    "env = GridWorld(width, height, terminal_states)\n",
    "agent = Agent(width, height, terminal_states)\n",
    "agent.render_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render_v_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy evaluation for 100 steps and 1 policy improvement step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    env.policy_evaluation_sweep(agent.policy)\n",
    "env.render_v_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_improvement(env)\n",
    "agent.render_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy evaluation for 100 steps with improved policy and again one policy improvement step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    env.policy_evaluation_sweep(agent.policy)\n",
    "env.render_v_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_improvement(env)\n",
    "agent.render_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
