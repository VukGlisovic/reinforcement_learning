{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "concerned-presentation",
   "metadata": {},
   "source": [
    "Code based on https://www.baeldung.com/cs/reinforcement-learning-neural-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-respondent",
   "metadata": {},
   "source": [
    "# The environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "state = row * ncol + col\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "\"\"\"\n",
    "\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(state):\n",
    "    return np.identity(env.observation_space.n)[state:state + 1]\n",
    "\n",
    "\n",
    "terminal_states = []\n",
    "for s in env.P.keys():  # for state\n",
    "    for a in env.P[s].keys():  # for action\n",
    "        prob, state, reward, terminal = env.P[s][a][0]\n",
    "        if terminal:\n",
    "            terminal_states.append(state)\n",
    "terminal_states = sorted(set(terminal_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-yesterday",
   "metadata": {},
   "source": [
    "# Agent setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.95\n",
    "eps = 0.4\n",
    "eps_decay_factor = 0.995\n",
    "num_episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.01)\n",
    "loss_fn = MeanSquaredError()\n",
    "\n",
    "inp = Input(shape=(env.observation_space.n,))\n",
    "x = Dense(20, activation='relu')(inp)\n",
    "out = Dense(env.action_space.n, activation='linear')(x)\n",
    "model = Model(inp, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies\n",
    "        # to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "        logits = model(inputs, training=True)  # Logits for this minibatch\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value = loss_fn(targets, logits)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    adam.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-depression",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    eps *= eps_decay_factor\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        if np.random.random() < eps:\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(model.predict(encode_state(state)))\n",
    "        new_state, reward, terminal, _ = env.step(action)\n",
    "        if new_state in terminal_states[:-1]:\n",
    "            # give a bit of negative reward to dying\n",
    "            reward = -0.05\n",
    "        if terminal:\n",
    "            target = reward # + discount_factor * 0.0\n",
    "            target_vector = np.zeros(shape=(env.action_space.n,))\n",
    "        else:\n",
    "            target = reward + discount_factor * np.max(model.predict(encode_state(new_state)))\n",
    "            target_vector = model.predict(encode_state(state))[0]\n",
    "        target_vector[action] = target\n",
    "        train_step(np.identity(env.observation_space.n)[state:state + 1], target_vector.reshape(-1, env.action_space.n))\n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-dallas",
   "metadata": {},
   "source": [
    "# Visualize learned q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = model.predict(np.identity(env.observation_space.n))\n",
    "q_table = pd.DataFrame(data=q_table, columns=['left', 'down', 'right', 'up'])\n",
    "q_table.index.name = 'state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap expected reward non-terminal states\n",
    "fig, ax = plt.subplots(figsize=(3, 6))\n",
    "\n",
    "sns.heatmap(q_table.loc[~q_table.index.isin(terminal_states)], annot=q_table.loc[~q_table.index.isin(terminal_states)], cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap expected reward all states\n",
    "fig, ax = plt.subplots(figsize=(3, 6))\n",
    "\n",
    "sns.heatmap(q_table, annot=q_table, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-investing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-admission",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
