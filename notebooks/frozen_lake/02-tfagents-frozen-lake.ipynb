{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "neural-bishop",
   "metadata": {},
   "source": [
    "Code based on https://www.baeldung.com/cs/reinforcement-learning-neural-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym, tf_py_environment, TimeLimit\n",
    "from tf_agents.environments.gym_wrapper import GymWrapper\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy, epsilon_greedy_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def now():\n",
    "    return datetime.now().strftime(\"%Y-%M-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-reserve",
   "metadata": {},
   "source": [
    "# Create the environment with slight adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeRewardOnDeadEnv(gym.Wrapper):\n",
    "    \"\"\"Gives a negative reward when the agent falls into the water.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(NegativeRewardOnDeadEnv, self).__init__(env)\n",
    "        self.dead_states = [5, 7, 11, 12]\n",
    "        self.gold_state = 15\n",
    "        \n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        if ob in self.dead_states:\n",
    "            reward = -1\n",
    "        if ob == self.gold_state:\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -1\n",
    "        return ob, reward, done, info\n",
    "\n",
    "\n",
    "env_name = 'FrozenLake-v1'\n",
    "train_gym_env = NegativeRewardOnDeadEnv(gym.make(env_name, is_slippery=False))\n",
    "eval_gym_env = NegativeRewardOnDeadEnv(gym.make(env_name, is_slippery=False))\n",
    "eval_gym_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = TimeLimit(GymWrapper(train_gym_env), duration=50)\n",
    "eval_py_env = TimeLimit(GymWrapper(eval_gym_env), duration=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Observation Spec:\\n', train_py_env.time_step_spec().observation)\n",
    "print('Reward Spec:\\n', train_py_env.time_step_spec().reward)\n",
    "print('Action Spec:\\n', train_py_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy arrays to tensors within the environment\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-rough",
   "metadata": {},
   "source": [
    "# Create the agent and its Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# network with one final Dense layer that use num_actions output nodes\n",
    "network_layers = [\n",
    "    tf.keras.layers.Lambda(lambda x: tf.one_hot(x, depth=16)),\n",
    "    tf.keras.layers.Dense(20, \n",
    "                          activation=tf.keras.activations.relu,\n",
    "                          name='input_layer'),\n",
    "    tf.keras.layers.Dense(train_py_env.action_spec().num_values, \n",
    "                          activation='linear', \n",
    "                          kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03), \n",
    "                          bias_initializer=tf.keras.initializers.Constant(0.0),\n",
    "                          name='output_layer')\n",
    "]\n",
    "\n",
    "q_net = sequential.Sequential(network_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    epsilon_greedy=1.1,\n",
    "    target_update_period=1000,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    gamma=0.99,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "agent._q_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-hours",
   "metadata": {},
   "source": [
    "# Setup policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy  # greedy policy\n",
    "collect_policy = agent.collect_policy  # epsilon-greedy policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())  # random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    total_steps = 0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            total_return += time_step.reward\n",
    "            total_steps += 1\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    avg_steps = total_steps / num_episodes\n",
    "    return avg_return.numpy()[0], avg_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average return under random policy\n",
    "compute_avg_return(eval_env, random_policy, num_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-aggregate",
   "metadata": {},
   "source": [
    "# Create replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_max_length = 10000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,  # train_env.batch_size=1\n",
    "    max_length=replay_buffer_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "        \n",
    "initial_collect_steps = 100\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2\n",
    ").prefetch(3)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-angola",
   "metadata": {},
   "source": [
    "### Deep dive in loss calculation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fifth-variety",
   "metadata": {},
   "source": [
    "experience, unused_info = next(iterator)\n",
    "experience"
   ]
  },
  {
   "cell_type": "raw",
   "id": "downtown-playback",
   "metadata": {},
   "source": [
    "loss_info = agent._loss(\n",
    "    experience,\n",
    "    td_errors_loss_fn=agent._td_errors_loss_fn,\n",
    "    gamma=agent._gamma,\n",
    "    reward_scale_factor=agent._reward_scale_factor,\n",
    "    weights=None,\n",
    "    training=True\n",
    ")\n",
    "loss_info"
   ]
  },
  {
   "cell_type": "raw",
   "id": "grave-concentration",
   "metadata": {},
   "source": [
    "transition = agent._as_transition(experience)\n",
    "time_steps, policy_steps, next_time_steps = transition\n",
    "actions = policy_steps.action\n",
    "transition"
   ]
  },
  {
   "cell_type": "raw",
   "id": "tight-productivity",
   "metadata": {},
   "source": [
    "q_values = agent._compute_q_values(time_steps, actions, training=True)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "persistent-client",
   "metadata": {},
   "source": [
    "next_q_values = agent._compute_next_q_values(next_time_steps, policy_steps.info)\n",
    "next_q_values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "undefined-scope",
   "metadata": {},
   "source": [
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "    \n",
    "td_targets = compute_td_targets(\n",
    "    next_q_values,\n",
    "    rewards=agent._reward_scale_factor * next_time_steps.reward,\n",
    "    discounts=agent._gamma * next_time_steps.discount\n",
    ")\n",
    "\n",
    "td_targets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "noted-printing",
   "metadata": {},
   "source": [
    "valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "td_error = valid_mask * (td_targets - q_values)\n",
    "td_error"
   ]
  },
  {
   "cell_type": "raw",
   "id": "difficult-patio",
   "metadata": {},
   "source": [
    "td_loss = valid_mask * agent._td_errors_loss_fn(td_targets, q_values)\n",
    "td_loss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "informed-comedy",
   "metadata": {},
   "source": [
    "agg_loss = common.aggregate_losses(\n",
    "    per_example_loss=td_loss,\n",
    "    sample_weight=None,\n",
    "    regularization_loss=agent._q_network.losses\n",
    ")\n",
    "total_loss = agg_loss.total_loss\n",
    "agg_loss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "tested-mistake",
   "metadata": {},
   "source": [
    "tf.reduce_mean(td_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-criminal",
   "metadata": {},
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_collect_policy_epsilon(agent, new_epsilon):\n",
    "    \"\"\"Utility function to update the collect_policies' epsilon.\n",
    "    \"\"\"\n",
    "    agent._epsilon_greedy = new_epsilon\n",
    "    agent._collect_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(agent.policy, epsilon=agent._epsilon_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 6000\n",
    "num_epsilon_greedy_steps = 3000\n",
    "num_eval_episodes = 10\n",
    "collect_steps_per_iteration = 5\n",
    "log_interval = 500\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return, avg_steps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns_lst = [avg_return]\n",
    "steps_lst = [avg_steps]\n",
    "\n",
    "print(\"[{}] Starting training...\".format(now()))\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "        \n",
    "    new_epsilon = max(agent._epsilon_greedy - 1 / num_epsilon_greedy_steps, 0.1)\n",
    "    update_collect_policy_epsilon(agent, new_epsilon)\n",
    "        \n",
    "    if step % log_interval == 0:\n",
    "        avg_return, avg_steps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print(\"[{}]\".format(now()) + f\" step = {step}: loss = {train_loss:<17,.10f} avg return = {avg_return:<10,.2f} avg steps = {avg_steps:.2f}\")\n",
    "        returns_lst.append(avg_return)\n",
    "        steps_lst.append(avg_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-voice",
   "metadata": {},
   "source": [
    "# Visualize q-values for all states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-cambridge",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, [ax, ax2] = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "iterations = list(range(0, len(returns_lst) * log_interval, log_interval))\n",
    "ax.plot(iterations, returns_lst, lw=2.5, alpha=0.8, label='returns')\n",
    "ax.set_ylabel('Average Return', fontsize=14)\n",
    "ax.set_xlabel('Gradient Steps', fontsize=14)\n",
    "ax.hlines(ax.get_yticks()[1:-1], iterations[0], iterations[-1], lw=0.5, alpha=0.5, ls='--', color='black')\n",
    "ax.legend(fontsize=13)\n",
    "\n",
    "ax2.plot(iterations, steps_lst, lw=1.5, alpha=0.7, color='black', label='game steps')\n",
    "ax2.set_ylabel('Steps per game', fontsize=14)\n",
    "ax2.set_xlabel('Gradient Steps', fontsize=14)\n",
    "ax2.hlines(ax2.get_yticks()[1:-1], iterations[0], iterations[-1], lw=0.5, alpha=0.5, ls='--', color='black')\n",
    "ax2.legend(fontsize=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = agent._q_network(np.arange(16))[0].numpy()\n",
    "q_table = pd.DataFrame(data=q_table, columns=['left', 'down', 'right', 'up'])\n",
    "q_table.index.name = 'state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap expected reward non-terminal states\n",
    "fig, ax = plt.subplots(figsize=(3, 6))\n",
    "\n",
    "terminal_states = eval_py_env.dead_states + [eval_py_env.gold_state]\n",
    "sns.heatmap(q_table.loc[~q_table.index.isin(terminal_states)], annot=q_table.loc[~q_table.index.isin(terminal_states)], cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap expected reward all states\n",
    "fig, ax = plt.subplots(figsize=(3, 7))\n",
    "\n",
    "sns.heatmap(q_table, annot=q_table, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-terrace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-flooring",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
