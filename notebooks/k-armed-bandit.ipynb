{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit(gym.Env):\n",
    "    \n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms        \n",
    "        self.m_low, self.m_high = 0, 5\n",
    "        self.s_low, self.s_high = 0.1, 0.3\n",
    "        \n",
    "        self.distributions, self.means, self.stds = self.get_distributions()\n",
    "        self.action_space = gym.spaces.Discrete(n_arms)\n",
    "        self.observation_space = gym.spaces.Discrete(1)  # there's only one state\n",
    "    \n",
    "    def env_start(self, seed):\n",
    "        np.random.seed(seed)  # set random seed for reward generation process\n",
    "        return self.observation_space.sample()  # select random sample for start\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = self.distributions[action]()\n",
    "        observation, terminal, info = None, False, dict()\n",
    "        return observation, reward, terminal, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.distributions, self.means, self.stds = self.get_distributions()\n",
    "    \n",
    "    def render(self):\n",
    "        x = np.linspace(self.m_low - 2*self.s_high, self.m_high + 2*self.s_high, 500)\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        for m, s in zip(self.means, self.stds):\n",
    "            ax.plot(x, norm.pdf(x, m, s), label=\"mean: {:.2f}, std: {:.2f}\".format(m, s))\n",
    "        ax.legend()\n",
    "    \n",
    "    def get_distributions(self):\n",
    "        np.random.seed(42)\n",
    "        means = np.random.uniform(low=self.m_low, high=self.m_high, size=self.n_arms)\n",
    "        stds = np.random.uniform(low=self.s_low, high=self.s_high, size=self.n_arms)\n",
    "        distributions = [lambda mi=mi, si=si: np.random.normal(mi, si) for mi, si in zip(means, stds)]\n",
    "        return distributions, means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.last_action = None\n",
    "        self.num_actions = None\n",
    "        self.q_values = None\n",
    "        self.step_size = None\n",
    "        self.epsilon = None\n",
    "        self.initial_value = 0.0\n",
    "\n",
    "    def agent_init(self, agent_setup):\n",
    "        self.n_actions = agent_setup['n_actions']\n",
    "        self.initial_value = agent_setup[\"initial_value\"]\n",
    "        self.q_values = np.ones(self.n_actions) * self.initial_value\n",
    "        self.step_size = agent_setup['step_size']\n",
    "        self.epsilon = agent_setup['epsilon']\n",
    "\n",
    "        self.last_action = 0\n",
    "        \n",
    "    @staticmethod\n",
    "    def argmax(q_values):\n",
    "        max_value = np.max(q_values)\n",
    "        max_indices = np.where(q_values == max_value)[0]\n",
    "        return np.random.choice(max_indices)\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            action = np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            action = self.argmax(self.q_values)\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, observation):\n",
    "        self.last_action = self.choose_action(observation)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, observation):\n",
    "        self.q_values[self.last_action] += self.step_size * (reward - self.q_values[self.last_action])\n",
    "        self.last_action = self.choose_action(observation)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        self.q_values[self.last_action] += self.step_size * (reward - self.q_values[self.last_action])\n",
    "\n",
    "    def agent_cleanup(self):\n",
    "        pass\n",
    "\n",
    "    def agent_message(self, message):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 4\n",
    "\n",
    "env_setup = {'n_arms': n_arms}\n",
    "agent_setup = {'n_actions': n_arms, 'initial_value': 10, 'step_size': 0.01, 'epsilon': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_expriment(agent_init, n_episodes=20, steps_per_episode=2000):\n",
    "    all_rewards = []\n",
    "    for e in tqdm(range(n_episodes)):\n",
    "        rewards = []\n",
    "        env = KArmedBandit(**env_setup)\n",
    "        observation = env.env_start(e)\n",
    "        agent = Agent()\n",
    "        agent.agent_init(agent_init)\n",
    "        action = agent.agent_start(observation)\n",
    "        observation, reward, terminal, info = env.step(action)\n",
    "        for step in range(steps_per_episode):\n",
    "            action = agent.agent_step(reward, observation)\n",
    "            observation, reward, terminal, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "        agent.agent_end(reward)\n",
    "        all_rewards.append(rewards)\n",
    "    return pd.DataFrame(np.array(all_rewards).T), env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards1, _ = run_expriment({'n_actions': n_arms, 'initial_value': 10, 'step_size': 0.01, 'epsilon': 0.01})\n",
    "df_rewards2, _ = run_expriment({'n_actions': n_arms, 'initial_value': 10, 'step_size': 0.1, 'epsilon': 0.1})\n",
    "df_rewards3, _ = run_expriment({'n_actions': n_arms, 'initial_value': 0, 'step_size': 0.1, 'epsilon': 0.1})\n",
    "_.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_result(df_rewards):\n",
    "    window = 50\n",
    "    average_reward = df_rewards.rolling(window=window).mean().mean(axis=1)\n",
    "    spread = df_rewards.rolling(window=window).mean().std(axis=1)\n",
    "    plus_spread = average_reward + spread\n",
    "    minus_spread = average_reward - spread\n",
    "    return average_reward, minus_spread, plus_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_performances(*dfs):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for i, df in enumerate(dfs, 1):\n",
    "        average_reward, minus_spread, plus_spread = aggregate_result(df)\n",
    "        ax.plot(average_reward)\n",
    "        ax.fill_between(average_reward.index, minus_spread, plus_spread, alpha=0.5, label=i)\n",
    "    ax.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performances(df_rewards1, df_rewards2, df_rewards3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
