{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-armed bandit environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit(gym.Env):\n",
    "    \"\"\"Creates a K-armed bandit environment. You specify the \n",
    "    number of arms you want and the environment is setup with\n",
    "    random (normally distributed) reward distributions for each \n",
    "    arm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms        \n",
    "        self.m_low, self.m_high = 0, 5\n",
    "        self.s_low, self.s_high = 0.1, 0.2\n",
    "        \n",
    "        self.distributions, self.means, self.stds = self.get_distributions()\n",
    "        self.action_space = gym.spaces.Discrete(n_arms)\n",
    "        self.observation_space = gym.spaces.Discrete(1)  # there's only one state\n",
    "    \n",
    "    def env_start(self, seed):\n",
    "        np.random.seed(seed)  # set random seed for reward generation process\n",
    "        return self.observation_space.sample()  # select random sample for start\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = self.distributions[action]()\n",
    "        observation, terminal, info = None, False, dict()\n",
    "        return observation, reward, terminal, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.distributions, self.means, self.stds = self.get_distributions()\n",
    "    \n",
    "    def render(self):\n",
    "        x = np.linspace(self.m_low - 2*self.s_high, self.m_high + 2*self.s_high, 500)\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        for m, s in zip(self.means, self.stds):\n",
    "            ax.plot(x, norm.pdf(x, m, s), label=\"mean: {:.2f}, std: {:.2f}\".format(m, s))\n",
    "        ax.legend()\n",
    "    \n",
    "    def get_distributions(self):\n",
    "        np.random.seed(42)\n",
    "        means = np.random.uniform(low=self.m_low, high=self.m_high, size=self.n_arms)\n",
    "        stds = np.random.uniform(low=self.s_low, high=self.s_high, size=self.n_arms)\n",
    "        distributions = [lambda mi=mi, si=si: np.random.normal(mi, si) for mi, si in zip(means, stds)]\n",
    "        return distributions, means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Creates an agent that can interact with the K-armed bandit\n",
    "    environment. You have to specify the number of arms present\n",
    "    in the environment class and you have to configure some learning\n",
    "    parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.last_action = None\n",
    "        self.num_actions = None\n",
    "        self.q_values = None\n",
    "        self.step_size = None\n",
    "        self.epsilon = None\n",
    "        self.initial_value = 0.0\n",
    "\n",
    "    def agent_init(self, agent_setup):\n",
    "        self.n_actions = agent_setup['n_actions']\n",
    "        self.initial_value = agent_setup[\"initial_value\"]\n",
    "        self.q_values = np.ones(self.n_actions) * self.initial_value\n",
    "        self.step_size = agent_setup['step_size']\n",
    "        self.epsilon = agent_setup['epsilon']\n",
    "\n",
    "        self.last_action = 0\n",
    "        \n",
    "    @staticmethod\n",
    "    def argmax(q_values):\n",
    "        max_value = np.max(q_values)\n",
    "        max_indices = np.where(q_values == max_value)[0]\n",
    "        return np.random.choice(max_indices)\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            action = np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            action = self.argmax(self.q_values)\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, observation):\n",
    "        self.last_action = self.choose_action(observation)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, observation):\n",
    "        self.q_values[self.last_action] += self.step_size * (reward - self.q_values[self.last_action])\n",
    "        self.last_action = self.choose_action(observation)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        self.q_values[self.last_action] += self.step_size * (reward - self.q_values[self.last_action])\n",
    "\n",
    "    def agent_cleanup(self):\n",
    "        pass\n",
    "\n",
    "    def agent_message(self, message):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env_init, agent_init, n_episodes=20, steps_per_episode=2000):\n",
    "    \"\"\"Runs an experiment for the specified number of episodes where\n",
    "    each episode runs a specified number of steps.\n",
    "    \"\"\"\n",
    "    all_rewards = []\n",
    "    all_q_value_estimates = []\n",
    "    for e in tqdm(range(n_episodes)):\n",
    "        rewards = []\n",
    "        env = KArmedBandit(**env_init)\n",
    "        observation = env.env_start(e)\n",
    "        agent = Agent()\n",
    "        agent.agent_init(agent_init)\n",
    "        action = agent.agent_start(observation)\n",
    "        observation, reward, terminal, info = env.step(action)\n",
    "        for step in range(steps_per_episode):\n",
    "            action = agent.agent_step(reward, observation)\n",
    "            observation, reward, terminal, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "        agent.agent_end(reward)\n",
    "        all_rewards.append(rewards)\n",
    "        all_q_value_estimates.append(agent.q_values)\n",
    "        \n",
    "    return pd.DataFrame(np.array(all_rewards).T), np.mean(np.array(all_q_value_estimates), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "Let's run the multiple experiments with different hyperparameters and compare. The experiments we run:\n",
    "\n",
    "1. optimistic initial value (10), mostly exploiting (epsilon=0.01), slow learner (step_size=0.01)\n",
    "2. optimistic initial value (10), more exploration (epsilon=0.1), fast learner (step_size=0.1)\n",
    "3. no optimistic initial value (0), mostly exploiting (epsilon=0.01), fast learner (step_size=0.1)\n",
    "4. no optimistic initial value (0), more exploration (epsilon=0.1), fast learner (step_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 4\n",
    "env_setup = {'n_arms': n_arms}\n",
    "env = KArmedBandit(**env_setup)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "history, agent1 = run_experiment(env_setup, {'n_actions': n_arms, 'initial_value': 10, 'step_size': 0.01, 'epsilon': 0.01})\n",
    "results.append(history)\n",
    "history, agent2 = run_experiment(env_setup, {'n_actions': n_arms, 'initial_value': 10, 'step_size': 0.1, 'epsilon': 0.1})\n",
    "results.append(history)\n",
    "history, agent3 = run_experiment(env_setup, {'n_actions': n_arms, 'initial_value': 0, 'step_size': 0.1, 'epsilon': 0.01})\n",
    "results.append(history)\n",
    "history, agent4 = run_experiment(env_setup, {'n_actions': n_arms, 'initial_value': 0, 'step_size': 0.1, 'epsilon': 0.1})\n",
    "results.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_result(df_rewards):\n",
    "    window = 50\n",
    "    average_reward = df_rewards.rolling(window=window).mean().mean(axis=1)\n",
    "    spread = df_rewards.rolling(window=window).mean().std(axis=1)\n",
    "    plus_spread = average_reward + spread\n",
    "    minus_spread = average_reward - spread\n",
    "    return average_reward, minus_spread, plus_spread\n",
    "\n",
    "\n",
    "def visualize_performances(*dfs):\n",
    "    fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "    for i, df in enumerate(dfs, 1):\n",
    "        average_reward, minus_spread, plus_spread = aggregate_result(df)\n",
    "        ax.plot(average_reward, label=\"Agent {}\".format(i))\n",
    "        # ax.fill_between(average_reward.index, minus_spread, plus_spread, alpha=0.3)\n",
    "    ax.legend(loc=4)\n",
    "    ax.set_xlabel(\"Step\", fontsize=14)\n",
    "    ax.set_ylabel(\"Average Reward\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performances(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_q_value_estimates(agent_estimates, env):\n",
    "    rounded_env_values = [round(v, 3) for v in env.means]\n",
    "    rounded_agent_values = [round(v, 3) for v in agent_estimates]\n",
    "    print(\"Environment mean rewards:\\t{}\".format(rounded_env_values))\n",
    "    print(\"Agent estimated q-values:\\t{}\".format(rounded_agent_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy agent with slow learning and optimistic initial values\n",
    "print_q_value_estimates(agent1, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nongreedy agent with fast learning and optimistic initial values\n",
    "print_q_value_estimates(agent2, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy agent with fast learning and no optimistic initial values\n",
    "print_q_value_estimates(agent3, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nongreedy agent with fast learning and no optimistic initial values\n",
    "print_q_value_estimates(agent4, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
