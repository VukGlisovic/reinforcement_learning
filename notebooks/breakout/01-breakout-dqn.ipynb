{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dressed-morris",
   "metadata": {},
   "source": [
    "# Using Deep Q-learning with tf-agents to solve breakout atari game\n",
    "\n",
    "![breakout_gif](https://i.imgur.com/rRxXF4H.gif \"breakout gif\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bridal-pattern",
   "metadata": {},
   "source": [
    "# for colab\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install tf-agents[reverb]\n",
    "!pip install pyglet\n",
    "\n",
    "# other notebook\n",
    "! apt install swig cmake libopenmpi-dev zlib1g-dev\n",
    "! pip install stable-baselines[mpi]==2.8.0 box2d box2d-kengz\n",
    "! pip install tensorflow==2.6.0\n",
    "! wget http://www.atarimania.com/roms/Roms.rar\n",
    "! mkdir /content/ROM/\n",
    "! unrar e /content/Roms.rar /content/ROM/\n",
    "! python -m atari_py.import_roms /content/ROM/\n",
    "! pip install tensorflow==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym, gym_wrapper, tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy, epsilon_greedy_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import PolicySaver\n",
    "\n",
    "from rl_src.atari_games.environment_preprocessing import wrap_atari_deepmind, hard_reset, get_timelimit_env\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Total memory: {}GB\".format(psutil.virtual_memory().total / (1024.0 ** 3)))\n",
    "\n",
    "\n",
    "def now():\n",
    "    return datetime.now().strftime(\"%Y-%M-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-smell",
   "metadata": {},
   "source": [
    "# Create and visualize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'BreakoutNoFrameskip-v4'\n",
    "train_gym_env = wrap_atari_deepmind(gym.make(env_name), frame_skip=4, frame_stack=True, scale=True, steps_limit=10000)\n",
    "eval_gym_env = wrap_atari_deepmind(gym.make(env_name), frame_skip=4, frame_stack=True, scale=True, steps_limit=10000)\n",
    "\n",
    "train_py_env = gym_wrapper.GymWrapper(train_gym_env)\n",
    "eval_py_env = gym_wrapper.GymWrapper(eval_gym_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Observation Spec:\\n', train_py_env.time_step_spec().observation)\n",
    "print('Reward Spec:\\n', train_py_env.time_step_spec().reward)\n",
    "print('Action Spec:\\n', train_py_env.action_spec())\n",
    "print(' Action meanings:', train_py_env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize game\n",
    "plt.imshow(train_py_env.render());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env.render().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that episodes run smoothly by running a number of environment steps with random actions\n",
    "\n",
    "df = pd.DataFrame(columns={'step_type': [], 'lives': [], 'reward': []})  # keep track of episode progress\n",
    "for i in range(1000):\n",
    "    obs = eval_py_env.step(np.random.randint(2, 4))  # random step left or right\n",
    "    new_point = {\n",
    "        'step_type': int(obs.step_type), \n",
    "        'lives': eval_py_env.env.unwrapped.ale.lives(),\n",
    "        'reward': float(obs.reward)\n",
    "    }\n",
    "    df = df.append(new_point, ignore_index=True)\n",
    "\n",
    "hard_reset(eval_py_env);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "df.plot(ax=ax, lw=1.5, alpha=0.75)\n",
    "ax.set_xlabel('Step', fontsize=16)\n",
    "ax.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each frame in one observation\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 6))\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    axes[i].imshow(obs.observation[:,:,i], cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot max pixel from each frame\n",
    "plt.imshow(np.max(obs.observation, axis=-1), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy arrays to tensors within the environment\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-lying",
   "metadata": {},
   "source": [
    "# Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "input_shape = train_py_env.observation_spec().shape\n",
    "print(\"Input shape: {}\".format(input_shape))\n",
    "# network with one final Dense layer that use num_actions output nodes\n",
    "network_layers = [\n",
    "    tf.keras.layers.InputLayer(input_shape, dtype=tf.float32, name='input'),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=8, strides=4, activation='relu', name='conv2d_1', dtype=tf.float32),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=2, activation='relu', name='conv2d_2', dtype=tf.float32),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, activation='relu', name='conv2d_3', dtype=tf.float32),\n",
    "    \n",
    "    tf.keras.layers.Flatten(name='flatten'),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation='relu', name='dense_1'),\n",
    "    tf.keras.layers.Dense(train_py_env.action_spec().num_values, activation='linear', name='dense_2')\n",
    "]\n",
    "\n",
    "q_net = sequential.Sequential(network_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    epsilon_greedy=1.1,\n",
    "    boltzmann_temperature=None,\n",
    "    target_update_period=10000,\n",
    "    td_errors_loss_fn=common.element_wise_huber_loss,\n",
    "    gamma=0.99,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "agent._q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy  # greedy policy\n",
    "collect_policy = agent.collect_policy  # epsilon-greedy policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())  # random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_collect_policy_epsilon(agent, new_epsilon):\n",
    "    \"\"\"Utility function to update the collect_policies' epsilon.\n",
    "    \"\"\"\n",
    "    agent._epsilon_greedy = new_epsilon\n",
    "    agent._collect_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(agent.policy, epsilon=agent._epsilon_greedy)\n",
    "\n",
    "\n",
    "def compute_avg_return(environment, policy, num_games=5):\n",
    "    \"\"\"Make a hard reset on the environment and play num_games.\n",
    "    \"\"\"\n",
    "    total_return = 0.0\n",
    "    total_steps = 0.0\n",
    "    get_info = environment.pyenv.get_info if isinstance(environment, tf_py_environment.TFPyEnvironment) else environment.get_info\n",
    "    get_lives = environment.pyenv.envs[0].unwrapped.ale.lives if isinstance(environment, tf_py_environment.TFPyEnvironment) else environment.unwrapped.ale.lives\n",
    "    time_limit_env = get_timelimit_env(environment)\n",
    "    \n",
    "    for _ in range(num_games):\n",
    "        time_step = hard_reset(environment)\n",
    "        game_return = 0.0\n",
    "        truncated = False\n",
    "        \n",
    "        while get_lives() > 0 and not truncated:\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            game_return += time_step.reward\n",
    "            info = get_info()\n",
    "            if isinstance(info, dict):\n",
    "                truncated = 'TimeLimit.truncated' in info.keys()\n",
    "            \n",
    "        total_return += game_return\n",
    "        total_steps += time_limit_env._elapsed_steps\n",
    "\n",
    "    avg_return = total_return / num_games\n",
    "    avg_steps = total_steps / num_games\n",
    "    return avg_return.numpy()[0], avg_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average return and number of steps under random policy\n",
    "compute_avg_return(eval_env, random_policy, num_games=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-lincoln",
   "metadata": {},
   "source": [
    "# Create replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_max_length = 100000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,  # train_env.batch_size=1\n",
    "    max_length=replay_buffer_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "    \n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "\n",
    "initial_collect_steps = 100\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2\n",
    ").prefetch(3)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-wrong",
   "metadata": {},
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000000\n",
    "num_epsilon_greedy_steps = 260000\n",
    "num_eval_games = 5\n",
    "collect_steps_per_iteration = 4  # update agent every collect_steps_per_iteration steps\n",
    "log_interval = 10000\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return, avg_steps = compute_avg_return(eval_env, agent.policy, num_eval_games)\n",
    "returns = [avg_return]\n",
    "steps = [avg_steps]\n",
    "\n",
    "best_return = 25  # start storing policies after reaching this amount of return\n",
    "print(\"[{}] Starting training...\".format(now()))\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "    \n",
    "    new_epsilon = max(agent._epsilon_greedy - 1.0 / num_epsilon_greedy_steps, 0.1)\n",
    "    update_collect_policy_epsilon(agent, new_epsilon)\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        avg_return, avg_steps = compute_avg_return(eval_env, agent.policy, num_eval_games)\n",
    "        print(\"[{}]\".format(now()) + f\" step = {step}: loss = {train_loss:<17,.10f} avg return = {avg_return:<10,.2f} avg steps = {avg_steps:.2f}\")\n",
    "        returns.append(avg_return)\n",
    "        steps.append(avg_steps)\n",
    "        if avg_return > best_return:\n",
    "            PolicySaver(eval_policy).save('breakout_agents/eval_policy_ret{:03d}_st{:04d}'.format(int(avg_return), (step // log_interval)))\n",
    "            best_return = avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax, ax2] = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "iterations = list(range(0, len(returns) * log_interval, log_interval))\n",
    "ax.plot(iterations, returns, lw=2.5, alpha=0.8, label='returns')\n",
    "\n",
    "window = 20\n",
    "rol_mean = [np.mean(returns[i-window: i]) for i in range(window, len(returns))]\n",
    "ax.plot(iterations[window:], rol_mean, lw=2.5, alpha=0.8, label='rolling mean returns')\n",
    "\n",
    "ax.set_ylabel('Average Return', fontsize=14)\n",
    "ax.set_xlabel('Gradient Steps', fontsize=14)\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.hlines(ax.get_yticks()[1:-1], iterations[0], iterations[-1], lw=0.5, alpha=0.5, ls='--', color='black')\n",
    "ax.legend(fontsize=13)\n",
    "\n",
    "ax2.plot(iterations, steps, lw=1.5, alpha=0.5, color='black', label='game steps')\n",
    "ax2.set_ylabel('Steps per game', fontsize=14)\n",
    "ax2.set_xlabel('Gradient Steps', fontsize=14)\n",
    "ax2.set_xlim(left=0)\n",
    "ax2.set_ylim(bottom=0)\n",
    "ax2.hlines(ax2.get_yticks()[1:-1], iterations[0], iterations[-1], lw=0.5, alpha=0.5, ls='--', color='black')\n",
    "ax2.legend(fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-excitement",
   "metadata": {},
   "source": [
    "# Show actions in video and see returns\n",
    "\n",
    "In the <a href=\"https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\">paper</a> they mention the following performance: ![breakout_performance](paper-breakout-performance.png \"Breakout Performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\n",
    "    \"\"\"\n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "\n",
    "def create_policy_eval_video(policy, filename, num_games=1, fps=30):\n",
    "    \"\"\"Uses eval_env and the provided policy to play games and make\n",
    "    a video of the gameplay.\n",
    "    \"\"\"\n",
    "    filename = filename + \".mp4\"\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        for _ in range(num_games):\n",
    "            total_reward = 0\n",
    "            get_info = eval_py_env.get_info\n",
    "            get_lives = eval_py_env.unwrapped.ale.lives\n",
    "            time_limit_env = get_timelimit_env(eval_py_env)\n",
    "            \n",
    "            time_step = hard_reset(eval_env)\n",
    "            video.append_data(eval_py_env.render())\n",
    "            truncated = False\n",
    "            while get_lives() > 0 and not truncated:\n",
    "                action_step = policy.action(time_step)\n",
    "                time_step = eval_env.step(action_step.action)\n",
    "                video.append_data(eval_py_env.render())\n",
    "                total_reward += time_step.reward.numpy()[0]\n",
    "                info = get_info()\n",
    "                if isinstance(info, dict):\n",
    "                    truncated = 'TimeLimit.truncated' in info.keys()\n",
    "            \n",
    "            print(\"{} steps with reward {}\".format(time_limit_env._elapsed_steps, total_reward))\n",
    "    return embed_mp4(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_policy_eval_video(random_policy, \"random-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load policy\n",
    "policy_path = 'breakout_agents/eval_policy_ret999_st9999/'\n",
    "policy = tf.saved_model.load(policy_path)\n",
    "\n",
    "create_policy_eval_video(policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-blade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-strip",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
